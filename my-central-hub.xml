This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.log, **/*.json, **/.gitignore, node_modules/**, thumbnails/**, **/dist/**, **/build/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
config/
  server.config.ts
core/
  httpParser.ts
  parser.ts
  router.ts
  server.ts
entities/
  http.ts
  sendResponse.ts
modules/
  app-metrics/
    app_gallery-generator/
      index.ts
      metricsController.ts
      metricsService.ts
    index.ts
    README.md
  embeddings/
    embedding.service.ts
    embeddings.handler.ts
  file-hosting/
    fileHostingController.ts
    fileHostingService.ts
    index.ts
  file-streaming/
    fileService.ts
    fileStreamingController.ts
    index.ts
routes/
  embeddings.routes.ts
  file-hosting.routes.ts
  files.routes.ts
  index.ts
  metrics.routes.ts
  stream.routes.ts
utils/
  helpers.ts
  httpHelpers.ts
  logger.ts
  mimeTypes.ts
embedding_service_helper.py
main.ts
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="config/server.config.ts">
import dotenv from 'dotenv';
import path, { join } from 'path';
import { Logger } from '../utils/logger';
// Instantiate logger using Logger class (default export mock was missing methods in tests)
const logger = new Logger();
// Load environment variables from .env file
dotenv.config();
export const config = {
  port: process.env.PORT ? parseInt(process.env.PORT, 10) : 8080,
  publicDir: process.env.PUBLIC_DIR
    ? join(process.cwd(), process.env.PUBLIC_DIR)
    : join(process.cwd(), 'public'),
  mediaDir: process.env.MEDIA_DIR
    ? join(process.cwd(), process.env.MEDIA_DIR)
    : join(process.cwd(), 'media'),
  headerTimeoutMs: process.env.HEADER_TIMEOUT_MS
    ? Math.max(parseInt(process.env.HEADER_TIMEOUT_MS, 10), 0)
    : 5000,
  bodyTimeoutMs: process.env.BODY_TIMEOUT_MS
    ? Math.max(parseInt(process.env.BODY_TIMEOUT_MS, 10), 0)
    : 10000,
  /**
   * Path to the SQLite database file. Will be created if missing.
   */
  dbPath: process.env.DB_PATH ? process.env.DB_PATH : join(process.cwd(), 'data', 'metrics.db'),
  /**
   * Feature toggles for modularity and configurability
   */
  features: {
    metrics: true,
    fileHosting: true,
    fileStreaming: true,
    // Add more features here as needed
  },
  /**
   * Logging configuration
   */
  logging: {
    level: process.env.LOG_LEVEL || 'info', // Default log level
    format: process.env.LOG_FORMAT || 'json', // Default log format
    logDir: process.env.LOG_DIR || join(process.cwd(), 'logs'), // Centralized log directory
  },
  /**
   * Embedder configuration
   */
  embedding: {
    // Python process settings
    pythonExecutable: process.env.PYTHON_EXECUTABLE || 'python3',
    pythonScriptPath:
      process.env.PYTHON_SCRIPT_PATH ||
      path.resolve(process.cwd(), 'python', 'embedding_service_helper.py'), // Path relative to project root
    pythonLogPath: process.env.PYTHON_LOG_PATH, // Optional: Path for python script's own log, defaults to alongside script if not set
    // Model/Processing Args passed to Python script
    modelArgs: [
      // Example: '--model', 'openai/clip-vit-base-patch32' is now expected to be set here
    ],
    defaultModel: 'openai/clip-vit-base-patch32', // Default model if not in args
    defaultNumFrames: 20,
    enableAugmentation: false, // Default augmentation flag for python script
    // Service behavior
    inactivityTimeoutMs: 5 * 60 * 1000,
    scriptTimeoutMs: 15 * 60 * 1000,
    debug: false,
    log: false,
  },
  testMode: true, // Set to true for testing purposes
};
// Log configuration only when not running tests
if (!config.testMode) {
  logger.info(`Server configuration:`);
  logger.info(`- Port: ${config.port}`);
  logger.info(`- Public Directory: ${config.publicDir}`);
  logger.info(`- Media Directory: ${config.mediaDir}`);
  logger.info(`- Header Timeout: ${config.headerTimeoutMs}ms`);
  logger.info(`- Body Timeout: ${config.bodyTimeoutMs}ms`);
  logger.info(`- Database Path: ${config.dbPath}`);
}
</file>

<file path="core/httpParser.ts">
import { IncomingRequest } from '../entities/http';
import { URL } from 'url';
import logger from '../utils/logger';
enum ParserState {
  REQUEST_LINE,
  HEADERS,
  BODY,
  CHUNK_SIZE,
  CHUNK_BODY,
  CHUNK_TRAILER,
  DONE,
  ERROR,
}
const ALLOWED_METHODS = ['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'HEAD', 'OPTIONS'];
const MAX_HEADER_BYTES = 8192; // 8KB
const MAX_HEADERS = 100; // Maximum number of headers allowed
const MAX_BODY_BYTES = 10 * 1024 * 1024; // 10 MB
const CRLF = Buffer.from('\r\n');
export class HttpRequestParser {
  protected buffer = Buffer.alloc(0);
  private state = ParserState.REQUEST_LINE;
  private headers: Record<string, string> = {};
  private headersMap = new Map<string, string[]>();
  private bodyChunks: Buffer[] = [];
  private method = '';
  private httpVersion = '';
  private url!: URL;
  private contentLength = 0;
  private remainingBody = 0;
  private isChunked = false;
  private invalid = false;
  private lastHeaderKey: string | null = null;
  /**
   * Returns the number of pending bytes in the parser buffer.
   */
  public getPendingBytes(): number {
    return this.buffer.length;
  }
  feed(data: Buffer): IncomingRequest | null {
    this.buffer = Buffer.concat([this.buffer, data]);
    try {
      while (true) {
        // REQUEST_LINE
        if (this.state === ParserState.REQUEST_LINE) {
          const idx = this.buffer.indexOf('\r\n');
          if (idx === -1) return null;
          const requestLine = this.buffer.subarray(0, idx).toString('utf8');
          this.buffer = this.buffer.subarray(idx + 2);
          const parts = requestLine.split(' ');
          if (parts.length !== 3) {
            this._setError('Invalid request line: ' + requestLine);
            return this._errorResponse();
          }
          const [method, reqPath, version] = parts;
          if (!ALLOWED_METHODS.includes(method)) {
            this._setError('Unsupported method: ' + method);
            continue;
          }
          if (!version.startsWith('HTTP/')) {
            this._setError('Invalid HTTP version: ' + version);
            continue;
          }
          this.method = method;
          this.httpVersion = version;
          try {
            this.url = new URL(reqPath, 'http://placeholder');
          } catch {
            this._setError('Malformed URL: ' + reqPath);
            continue;
          }
          this.state = ParserState.HEADERS;
        }
        // HEADERS
        if (this.state === ParserState.HEADERS) {
          // find end-of-headers or handle empty header block
          const idx = this.buffer.indexOf('\r\n\r\n');
          let headersRaw = '';
          if (idx === -1) {
            // immediate blank line → zero headers
            if (this.buffer.subarray(0, 2).equals(CRLF)) {
              this.buffer = this.buffer.subarray(2);
            } else {
              if (this.buffer.length > MAX_HEADER_BYTES) {
                this._setError('Headers too large');
                return this._errorResponse();
              }
              return null;
            }
          } else {
            headersRaw = this.buffer.subarray(0, idx).toString('utf8');
            this.buffer = this.buffer.subarray(idx + 4);
          }
          const lines = headersRaw.split('\r\n');
          let headerCount = 0;
          for (const line of lines) {
            if (line.trim() === '') continue;
            // support folded headers per RFC7230 §3.2.4
            if (line.startsWith(' ') || line.startsWith('\t')) {
              if (this.lastHeaderKey) {
                const prev = this.headers[this.lastHeaderKey];
                this.headers[this.lastHeaderKey] = prev + ' ' + line.trim();
                // update map too
                this.headersMap.set(
                  this.lastHeaderKey,
                  (this.headersMap.get(this.lastHeaderKey) || []).concat(
                    this.headers[this.lastHeaderKey],
                  ),
                );
                continue;
              } else {
                this._setError('Invalid header folding');
                return this._errorResponse();
              }
            }
            const colon = line.indexOf(':');
            if (colon === -1) {
              this._setError('Invalid header line: ' + line);
              continue;
            }
            const key = line.slice(0, colon).trim().toLowerCase();
            const value = line.slice(colon + 1).trim();
            this.headers[key] = value;
            this.headersMap.set(key, [...(this.headersMap.get(key) ?? []), value]);
            this.lastHeaderKey = key;
            headerCount++;
            if (headerCount > MAX_HEADERS) {
              this._setError('Too many headers');
              return this._errorResponse();
            }
          }
          // Short-circuit on parse errors
          if (this.invalid) {
            return this._errorResponse();
          }
          // enforce Host header for HTTP/1.1
          if (this.httpVersion === 'HTTP/1.1' && !this.headers['host']) {
            this._setError('Missing Host header');
            return this._errorResponse();
          }
          if (
            this.headers['transfer-encoding'] &&
            this.headers['transfer-encoding'].toLowerCase() === 'chunked'
          ) {
            this.isChunked = true;
            this.state = ParserState.CHUNK_SIZE;
          } else if (this.headers['content-length']) {
            this.contentLength = parseInt(this.headers['content-length'], 10);
            if (isNaN(this.contentLength) || this.contentLength < 0) {
              this._setError('Invalid Content-Length');
              continue;
            }
            this.remainingBody = this.contentLength;
            this.state = this.contentLength > 0 ? ParserState.BODY : ParserState.DONE;
          } else {
            this.state = ParserState.DONE;
          }
        }
        // BODY
        if (this.state === ParserState.BODY) {
          this.contentLength = parseInt(this.headers['content-length'], 10);
          if (isNaN(this.contentLength) || this.contentLength < 0) {
            this._setError('Invalid Content-Length');
            continue;
          }
          if (this.contentLength > MAX_BODY_BYTES) {
            this._setError('Request body too large');
            continue;
          }
          this.remainingBody = this.contentLength;
          if (this.buffer.length < this.remainingBody) return null;
          this.bodyChunks.push(this.buffer.subarray(0, this.remainingBody));
          this.buffer = this.buffer.subarray(this.remainingBody);
          this.remainingBody = 0;
          this.state = ParserState.DONE;
        }
        // CHUNK_SIZE
        if (this.state === ParserState.CHUNK_SIZE) {
          const idx = this.buffer.indexOf('\r\n');
          if (idx === -1) return null;
          const line = this.buffer.subarray(0, idx).toString('utf8');
          this.buffer = this.buffer.subarray(idx + 2);
          const chunkSize = parseInt(line, 16);
          if (isNaN(chunkSize)) {
            this._setError('Invalid chunk size: ' + line);
            continue;
          }
          if (chunkSize === 0) {
            this.state = ParserState.CHUNK_TRAILER;
          } else {
            this.remainingBody = chunkSize;
            this.state = ParserState.CHUNK_BODY;
          }
        }
        // CHUNK_BODY
        if (this.state === ParserState.CHUNK_BODY) {
          if (this.remainingBody > MAX_BODY_BYTES) {
            this._setError('Chunk body too large');
            continue;
          }
          if (this.buffer.length < this.remainingBody) return null;
          const chunk = this.buffer.subarray(0, this.remainingBody);
          this.bodyChunks.push(chunk);
          this.buffer = this.buffer.subarray(this.remainingBody);
          this.remainingBody = 0;
          if (this.buffer.length < 2 || !this.buffer.subarray(0, 2).equals(CRLF)) {
            this._setError('Missing CRLF after chunk');
            continue;
          }
          this.buffer = this.buffer.subarray(2);
          this.state = ParserState.CHUNK_SIZE;
        }
        // CHUNK_TRAILER
        if (this.state === ParserState.CHUNK_TRAILER) {
          if (this.buffer.length === 0 || this.buffer.equals(CRLF)) {
            this.buffer = Buffer.alloc(0);
            this.state = ParserState.DONE;
            continue;
          }
          const idx = this.buffer.indexOf('\r\n\r\n');
          if (idx === -1) return null;
          this.buffer = this.buffer.subarray(idx + 4);
          this.state = ParserState.DONE;
        }
        // DONE
        if (this.state === ParserState.DONE) {
          // capture leftover before reset (for pipelining)
          const leftover = this.buffer;
          const finalBody = this.bodyChunks.length ? Buffer.concat(this.bodyChunks) : undefined;
          const request = {
            method: this.method,
            path: this.url.pathname,
            query: Object.fromEntries(this.url.searchParams.entries()),
            headers: this.headers,
            headersMap: this.headersMap,
            httpVersion: this.httpVersion,
            url: this.url,
            body: finalBody,
            raw: '',
            ctx: {},
            invalid: this.invalid,
          };
          this.reset();
          this.buffer = leftover; // restore leftover for next request
          return request;
        }
        // ERROR
        if (this.state === ParserState.ERROR) {
          const errReq = this._errorResponse();
          this.reset();
          return errReq;
        }
      }
    } catch (err) {
      this._setError(`Error during parsing: ${(err as Error).message}`);
      const errReq = this._errorResponse();
      this.reset();
      return errReq;
    }
  }
  private _setError(message: string): void {
    logger.error(message);
    this.invalid = true;
    this.state = ParserState.ERROR;
  }
  /** Build a minimal invalid IncomingRequest */
  private _errorResponse(): IncomingRequest {
    return {
      method: '',
      path: '',
      query: {},
      headers: {},
      headersMap: new Map(),
      httpVersion: '',
      url: new URL('http://invalid'),
      body: undefined,
      raw: '',
      ctx: {},
      invalid: true,
    };
  }
  reset(): void {
    this.buffer = Buffer.alloc(0);
    this.state = ParserState.REQUEST_LINE;
    this.headers = {};
    this.headersMap = new Map();
    this.bodyChunks = [];
    this.method = '';
    this.httpVersion = '';
    this.url = new URL('http://placeholder');
    this.contentLength = 0;
    this.remainingBody = 0;
    this.isChunked = false;
    this.invalid = false;
    this.lastHeaderKey = null;
  }
}
</file>

<file path="core/parser.ts">
import { IncomingRequest } from '../entities/http';
import { URL } from 'url';
const ALLOWED_METHODS = ['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'HEAD', 'OPTIONS'] as const;
const MAX_HEADERS = 1000;
// helper for duplicate headers
function addHeader(map: Map<string, string[]>, key: string, value: string) {
  const k = key.toLowerCase();
  const list = map.get(k) ?? [];
  list.push(value);
  map.set(k, list);
}
export const parser = {
  parse(raw: string): IncomingRequest {
    const dummyUrl = new URL('http://placeholder/');
    const earlyReturn = (): IncomingRequest => ({
      url: dummyUrl,
      path: '',
      query: {},
      httpVersion: '',
      method: '',
      headers: {},
      headersMap: new Map(),
      raw,
      ctx: {},
      invalid: true,
    });
    /* -------- empty buffer guard -------- */
    if (raw.length === 0) return earlyReturn();
    const [head = '', bodyString = ''] = raw.split('\r\n\r\n');
    const lines = head.split('\r\n');
    /* -------- request-line split -------- */
    const [requestLine, ...headerLines] = lines;
    const parts = requestLine.split(' ');
    if (parts.length < 3) return earlyReturn();
    const [method, fullPath, httpVersion] = parts;
    const isOptionsStar = method === 'OPTIONS' && fullPath === '*';
    let invalid =
      !ALLOWED_METHODS.includes(method as (typeof ALLOWED_METHODS)[number]) ||
      (!fullPath.startsWith('/') && !isOptionsStar) ||
      !httpVersion.startsWith('HTTP/');
    /* -------- URL + query -------- */
    const url = isOptionsStar
      ? new URL('http://placeholder')
      : new URL(fullPath, 'http://placeholder');
    const query: Record<string, string> = {};
    url.searchParams.forEach((v, k) => (query[k] = v));
    /* -------- headers -------- */
    const headers: Record<string, string> = {};
    const headersMap = new Map<string, string[]>();
    if (headerLines.length > MAX_HEADERS) invalid = true;
    for (const line of headerLines) {
      const idx = line.indexOf(':');
      if (idx === -1) {
        invalid = true;
        continue;
      }
      const key = line.slice(0, idx).trim();
      const value = line.slice(idx + 1).trim();
      headers[key.toLowerCase()] = value;
      addHeader(headersMap, key, value);
    }
    const body = bodyString ? Buffer.from(bodyString, 'utf-8') : undefined;
    return {
      url,
      path: isOptionsStar ? '*' : decodeURIComponent(url.pathname),
      query,
      httpVersion,
      method,
      headers,
      headersMap,
      body,
      raw,
      ctx: {},
      invalid,
    };
  },
};
</file>

<file path="core/router.ts">
// src/core/router.ts
import { Socket } from 'net';
import { IncomingRequest } from '../entities/http';
import { sendResponse } from '../entities/sendResponse';
import { Logger, ConsoleTransport, FileTransport, PrettyFormatter } from '../utils/logger';
import path from 'path';
import { config } from '../config/server.config';
const logger = new Logger({
  transports: [
    new ConsoleTransport({
      formatter: new PrettyFormatter(),
      level: 'info',
    }),
    new FileTransport({
      filename: path.join(
        (config.logging && config.logging.logDir) || path.join(process.cwd(), 'logs'),
        'router.log',
      ),
      formatter: new PrettyFormatter(),
      level: 'debug',
    }),
  ],
  level: 'info',
  exitOnError: false,
});
/* ───── Types ─────────────────────────────────────────────────────────── */
export type Handler = (req: IncomingRequest, sock: Socket) => Promise<void> | void;
export type Middleware = (
  req: IncomingRequest,
  sock: Socket,
  next: () => Promise<void>,
) => Promise<void> | void;
interface Route {
  method: string; // 'GET' | 'POST' | 'ANY'
  regex: RegExp; // compiled path matcher
  keys: string[]; // param names (for :id stuff)
  handler: Handler;
}
/* ───── Router implementation ─────────────────────────────────────────── */
class Router {
  private middlewares: Middleware[] = [];
  private routes: Route[] = [];
  /* ---------- Middleware ---------- */
  use(mw: Middleware) {
    this.middlewares.push(mw);
  }
  /* ---------- Route registration helpers ---------- */
  add(method: string, path: string, handler: Handler) {
    const { regex, keys } = compilePath(path);
    this.routes.push({ method: method.toUpperCase(), regex, keys, handler });
  }
  get(path: string, h: Handler) {
    this.add('GET', path, h);
  }
  post(path: string, h: Handler) {
    this.add('POST', path, h);
  }
  put(path: string, h: Handler) {
    this.add('PUT', path, h);
  }
  del(path: string, h: Handler) {
    this.add('DELETE', path, h);
  }
  any(path: string, h: Handler) {
    this.add('ANY', path, h);
  }
  /* ---------- Main entry ---------- */
  async handle(req: IncomingRequest, sock: Socket): Promise<void> {
    if (req.method === 'OPTIONS') {
      sendResponse(
        sock,
        200,
        { 'Content-Type': 'text/plain', Allow: 'GET, POST, PUT, DELETE, OPTIONS' },
        'OK',
      );
      return;
    }
    if (!req.path || typeof req.path !== 'string') {
      sendResponse(
        sock,
        400,
        {
          'Content-Type':
            req.path && req.path.startsWith('/api/') ? 'application/json' : 'text/plain',
        },
        req.path && req.path.startsWith('/api/')
          ? JSON.stringify({ error: 'Bad Request' })
          : 'Bad Request',
      );
      return;
    }
    logger.info(`router saw ${req.method} ${req.path}`);
    // 1. run middleware chain
    let i = 0;
    // Use an explicit stack to unwind after handler
    const run = async (): Promise<void> => {
      if (i < this.middlewares.length) {
        const idx = i++;
        await this.middlewares[idx](req, sock, run);
        return;
      }
      // Only after all middleware, run the handler
      // 2. route lookup
      const matching = this.routes.filter((r) => r.regex.test(req.path));
      const route =
        matching.find((r) => r.method === req.method) ?? matching.find((r) => r.method === 'ANY');
      if (!route) {
        // Distinguish 404 vs 405
        if (matching.length) {
          // Only include allowed methods that are not 'ANY'
          const allowed = matching
            .map((r) => r.method)
            .filter((m) => m !== 'ANY')
            .join(', ');
          if (req.path.startsWith('/api/')) {
            sendResponse(
              sock,
              405,
              {
                'Content-Type': 'application/json',
                Allow: allowed,
              },
              JSON.stringify({ error: 'Method Not Allowed' }),
            );
          } else {
            sendResponse(sock, 405, { Allow: allowed }, 'Method Not Allowed');
          }
        } else {
          if (req.path.startsWith('/api/')) {
            sendResponse(
              sock,
              404,
              { 'Content-Type': 'application/json' },
              JSON.stringify({ error: 'Not Found' }),
            );
          } else {
            sendResponse(sock, 404, { 'Content-Type': 'text/plain' }, 'Not Found');
          }
        }
        return;
      }
      // 3. pull params (/:id etc.) → req.ctx.params
      const match = route.regex.exec(req.path)!;
      const params: Record<string, string> = {};
      route.keys.forEach((k, idx) => {
        params[k] = decodeURIComponent(match[idx + 1]);
      });
      (req.ctx ??= {}).params = params;
      // 4. invoke handler
      try {
        await route.handler(req, sock);
      } catch (err) {
        logger.error(`Handler error: ${(err as Error).message}`);
        if (req.path.startsWith('/api/')) {
          sendResponse(
            sock,
            500,
            { 'Content-Type': 'application/json' },
            JSON.stringify({ error: 'Internal Server Error' }),
          );
        } else {
          sendResponse(sock, 500, { 'Content-Type': 'text/plain' }, '500 Server Error');
        }
      }
    };
    try {
      await run();
    } catch (err) {
      // Global error handler for middleware
      if (req.path.startsWith('/api/')) {
        sendResponse(
          sock,
          500,
          { 'Content-Type': 'application/json' },
          JSON.stringify({ error: 'Internal Server Error' }),
        );
      } else {
        sendResponse(sock, 500, { 'Content-Type': 'text/plain' }, '500 Server Error');
      }
      logger.error(`Middleware error: ${(err as Error).message}`);
      return;
    }
  }
}
/* ───── Path pattern compiler ───────────────────────────────────────────
   /files/:name  ->  ^/files/([^/]+)$           keys=['name']
   /api/*        ->  ^/api/(.*)$                keys=['*']
------------------------------------------------------------------------ */
function compilePath(pattern: string): { regex: RegExp; keys: string[] } {
  const keys: string[] = [];
  const regexSrc = pattern
    .replace(/\/:(\w+)/g, (_, k) => {
      keys.push(k);
      return '/([^/]+)';
    })
    .replace(/\*/g, () => {
      keys.push('*');
      return '(.*)';
    });
  return { regex: new RegExp(`^${regexSrc}$`), keys };
}
/* ───── Exports ───────────────────────────────────────────────────────── */
export function createRouter(): Router {
  return new Router();
}
export default createRouter();
</file>

<file path="core/server.ts">
import { createServer, Socket } from 'net';
import { HttpRequestParser } from './httpParser';
import router from './router';
import logger from '../utils/logger';
import { sendResponse } from '../entities/sendResponse';
import { config } from '../config/server.config'; // Assuming config is imported from a config file
export class HttpServer {
  private server = createServer();
  private readonly connections = new Set<Socket>();
  private readonly router;
  constructor(
    private port: number,
    routerInstance = router,
  ) {
    this.router = routerInstance;
    this.setupServer();
  }
  private setupServer() {
    this.server.on('connection', (socket: Socket) => {
      this.connections.add(socket);
      const parser = new HttpRequestParser();
      // --- ⏰ Idle Timeout (Protection) ---
      const HEADER_TIMEOUT_MS = config.headerTimeoutMs;
      const BODY_TIMEOUT_MS = config.bodyTimeoutMs;
      let headerTimer: NodeJS.Timeout | undefined;
      let bodyTimer: NodeJS.Timeout | undefined;
      const refreshTimeout = () => {
        if (headerTimer) clearTimeout(headerTimer);
        headerTimer = setTimeout(() => {
          logger.warn('Closing idle socket (header timeout)');
          socket.destroy();
        }, HEADER_TIMEOUT_MS);
      };
      const refreshBodyTimeout = () => {
        if (bodyTimer) clearTimeout(bodyTimer);
        bodyTimer = setTimeout(() => {
          logger.warn('Closing idle socket (body timeout)');
          socket.destroy();
        }, BODY_TIMEOUT_MS);
      };
      refreshTimeout(); // start immediately
      socket.once('close', () => {
        this.connections.delete(socket);
        if (headerTimer) clearTimeout(headerTimer);
        if (bodyTimer) clearTimeout(bodyTimer);
      });
      logger.info('New connection established.');
      socket.on('data', async (chunk: Buffer) => {
        refreshTimeout();
        try {
          // First feed yields the first complete request (or null)
          let req = parser.feed(chunk);
          if (!req) return; // need more bytes
          clearTimeout(headerTimer);
          // Handle all pipelined requests in buffer
          do {
            // If this is a body-bearing method, start body timeout
            if (req.method === 'POST' || req.method === 'PUT' || req.method === 'PATCH') {
              refreshBodyTimeout();
            }
            await this.router.handle(req, socket);
            clearTimeout(bodyTimer);
            // now pull the next request from any leftover bytes
            req = parser.feed(Buffer.alloc(0));
          } while (req);
          const pending = parser.getPendingBytes();
          if (pending > 0) {
            // more pipelined data waiting—keep alive
            refreshTimeout();
          } else {
            socket.end();
          }
        } catch (err) {
          logger.error(`Failed request: ${(err as Error).message}`);
          sendResponse(socket, 400, { 'Content-Type': 'text/plain' }, 'Bad Request');
        }
      });
      socket.on('error', (err) => {
        logger.error(`Socket error: ${err.message}`);
      });
    });
    this.server.on('error', (err: NodeJS.ErrnoException) => {
      logger.error(`Server error:`, [err, err.code, err.message]);
      // if (err.code === 'EADDRINUSE') {
      //   this.port += 1; // try the next port
      //   logger.warn(`Port busy, retrying on ${this.port}`);
      //   this.server.listen(this.port);
      // } else {
      //   logger.error(`Server error: ${err.message}`);
      // }
    });
  }
  /**
   * Gracefully shuts down the server and every open TCP socket.
   */
  public async stop(): Promise<void> {
    logger.info('🛑  Shutting down HTTP server');
    for (const sock of this.connections) sock.destroy();
    await new Promise<void>((resolve, reject) =>
      this.server.close((err) => (err ? reject(err) : resolve())),
    );
  }
  /**
   * Public method to destroy all active sockets.
   */
  public destroySockets(): void {
    this.connections.forEach((socket) => socket.destroy());
  }
  public start() {
    this.server.listen(this.port, () => {
      logger.info(`🚀 Server running at port ${this.port}`);
    });
    // graceful shutdown on Ctrl-C / kill
    ['SIGINT', 'SIGTERM'].forEach((sig) =>
      process.on(sig as NodeJS.Signals, () => {
        this.stop()
          .then(() => process.exit(0))
          .catch(() => process.exit(1));
      }),
    );
    // src/core/server.ts  – inside start() after existing SIGINT/SIGTERM hooks
    ['SIGUSR2'].forEach((sig) =>
      process.once(sig as NodeJS.Signals, () => {
        this.stop().then(() => process.kill(process.pid, sig));
      }),
    );
  }
}
</file>

<file path="entities/http.ts">
export interface IncomingRequest {
  url: URL; // canonical URL (always present)
  path: string; // == url.pathname
  query: Record<string, string>; // decoded single-value map
  httpVersion: string; // e.g. "HTTP/1.1"
  method: string;
  headers: Record<string, string>;
  headersMap?: Map<string, string[]>;
  body?: Buffer;
  raw: string;
  ctx?: Record<string, unknown>;
  invalid?: boolean;
}
</file>

<file path="entities/sendResponse.ts">
import { Socket } from 'net';
import { Readable } from 'stream';
const STATUS_TEXT: Record<number, string> = {
  200: 'OK',
  206: 'Partial Content',
  400: 'Bad Request',
  404: 'Not Found',
  405: 'Method Not Allowed',
  416: 'Range Not Satisfiable', // ← new
  500: 'Internal Server Error',
};
export function sendResponse(
  socket: Socket,
  status: number,
  headers: Record<string, string>,
  body?: string | Buffer | Readable,
): void {
  const head =
    `HTTP/1.1 ${status} ${STATUS_TEXT[status] ?? ''}\r\n` +
    Object.entries(headers)
      .map(([k, v]) => `${k}: ${v}`)
      .join('\r\n') +
    '\r\n\r\n';
  socket.write(head);
  if (!body) {
    // No body: write head only, leave socket open for HttpServer to manage closing
    return;
  }
  if (body instanceof Readable) {
    console.log('[DEBUG] sendResponse: piping stream');
    body.once('error', (err) => {
      console.error('[DEBUG] sendResponse: stream error caught', err.message);
      if (!socket.destroyed) socket.destroy();
    });
    body.pipe(socket, { end: false });
    return;
  }
  // Write body without closing socket
  socket.write(body);
}
</file>

<file path="modules/app-metrics/app_gallery-generator/index.ts">
export * from './metricsController';
export * from './metricsService';
</file>

<file path="modules/app-metrics/app_gallery-generator/metricsController.ts">
import { IncomingRequest } from '../../../entities/http';
import { sendResponse } from '../../../entities/sendResponse';
import { Socket } from 'net';
import { saveMetrics, isPerfLogArray } from './metricsService';
import logger from '../../../utils/logger';
export const metricsController = {
  handleMetrics: async (req: IncomingRequest, sock: Socket) => {
    try {
      if (req.method !== 'POST') {
        sendResponse(
          sock,
          405,
          {
            'Content-Type': 'text/plain',
          },
          'Method Not Allowed',
        );
        return;
      }
      let payload: unknown;
      try {
        // Accept Buffer, string, or already-parsed object
        if (Buffer.isBuffer(req.body)) {
          payload = JSON.parse(req.body.toString('utf8'));
        } else if (typeof req.body === 'string') {
          payload = JSON.parse(req.body);
        } else if (typeof req.body === 'object' && req.body !== null) {
          payload = req.body;
        } else {
          payload = {};
        }
      } catch (err) {
        logger.info(`[metrics] Invalid JSON: ${err}`);
        sendResponse(
          sock,
          400,
          { 'Content-Type': 'application/json' },
          JSON.stringify({ error: 'Invalid JSON', details: String(err) }),
        );
        return;
      }
      const clientSessionId = req.headers['x-session-id'] || req.headers['X-Session-Id'];
      // PerfLog batch shortcut
      if (Array.isArray(payload) && isPerfLogArray(payload)) {
        await saveMetrics(payload, clientSessionId);
        sendResponse(
          sock,
          200,
          { 'Content-Type': 'application/json' },
          JSON.stringify({ status: 'OK', payload: payload, message: 'valid payload' }),
        );
        return;
      }
      // Always call saveMetrics, even if partially invalid
      await saveMetrics(payload, clientSessionId);
      sendResponse(
        sock,
        200,
        { 'Content-Type': 'application/json' },
        JSON.stringify({ status: 'OK', payload: payload, message: 'possible invalid payload' }),
      );
    } catch (err) {
      logger.info(`[metrics] error saving payload: ${err}`);
      sendResponse(
        sock,
        500,
        {
          'Content-Type': 'text/plain',
        },
        Buffer.from(JSON.stringify({ message: 'Internal Server Error', error: err })),
      );
    }
  },
};
</file>

<file path="modules/app-metrics/app_gallery-generator/metricsService.ts">
import { z } from 'zod';
import sqlite3 from 'sqlite3';
import { open, Database } from 'sqlite';
import { config } from '../../../config/server.config';
import { Logger, FileTransport, ConsoleTransport, PrettyFormatter } from '../../../utils/logger';
import fs from 'fs';
import path from 'path';
const logger = new Logger({
  transports: [
    new ConsoleTransport({
      formatter: new PrettyFormatter(),
      level: 'info',
    }),
    new FileTransport({
      filename: path.join(config.logging.logDir, 'metricsService.log'),
      formatter: new PrettyFormatter(),
      level: 'debug',
    }),
  ],
});
// --- Zod Schemas ---
const PerfLogEntrySchema = z
  .object({
    timestamp: z.string(),
    perfNow: z.number(),
    memory: z
      .object({
        usedJSHeapSize: z.number().optional(),
        totalJSHeapSize: z.number().optional(),
        jsHeapSizeLimit: z.number().optional(),
      })
      .optional()
      .nullable(), // allow null as well as undefined
    action: z.string(),
    sessionId: z.string().optional(),
    batchId: z.number().optional(),
    uploadMode: z.string().optional(),
  })
  .catchall(z.unknown());
const EngagementStatsSchema = z.object({
  views: z.number(),
  lastViewedAt: z.number(),
  totalWatchMs: z.number(),
  completions: z.number(),
});
const EngagementMapSchema = z.record(EngagementStatsSchema);
const DebugLogSchema = z.object({
  message: z.string(),
  timestamp: z.string(),
});
const DebugLogsSchema = z.array(DebugLogSchema);
const SessionMetricsSchema = z.object({
  sessionId: z.string(),
  startTime: z.number(),
  firstClickTime: z.number().optional(),
  scrollDistance: z.number(),
  itemsLoaded: z.number(),
  infiniteScrollLoads: z.number(),
  hoverThumbnails: z.number(),
  gridClickOpenCount: z.number(),
  modalsOpened: z.number(),
  modalTotalTime: z.number(),
  carouselNavigationCount: z.number(),
  modalContentCounts: z.object({
    video: z.number(),
    image: z.number(),
  }),
  videoMetrics: z.object({
    plays: z.number(),
    completions: z.number(),
    watchTime: z.number(),
    manualStarts: z.number(),
    autoPlays: z.number(),
  }),
  performanceMetrics: z.object({
    preloadDurations: z.array(z.number()),
    longTasks: z.number(),
    infiniteLoadTimes: z.array(z.number()),
    modalAnimationLatencies: z.array(z.number()),
  }),
});
const FullPayloadSchema = z
  .object({
    engagement: EngagementMapSchema,
    perfLog: z.array(PerfLogEntrySchema),
    debug: DebugLogsSchema,
    timestamp: z.string(),
    sessionStart: z.number(),
    sessionMetrics: SessionMetricsSchema,
  })
  .catchall(z.unknown());
export type PerfLogEntry = z.infer<typeof PerfLogEntrySchema>;
export type FullPayload = z.infer<typeof FullPayloadSchema>;
// Export metricsPayloadSchema for controller usage
export const metricsPayloadSchema = FullPayloadSchema;
// Type guard for PerfLogEntry array
export function isPerfLogArray(arr: unknown): arr is PerfLogEntry[] {
  return Array.isArray(arr) && arr.every((item) => PerfLogEntrySchema.safeParse(item).success);
}
// --- DB Setup ---
let db: Database | null = null;
export async function initDb(): Promise<Database> {
  if (db) return db;
  const dbDir = path.dirname(config.dbPath);
  if (!fs.existsSync(dbDir)) {
    fs.mkdirSync(dbDir, { recursive: true });
    logger.info(`Created database directory at ${dbDir}`);
  }
  if (!fs.existsSync(config.dbPath)) {
    fs.closeSync(fs.openSync(config.dbPath, 'w'));
    logger.info(`Created new SQLite database file at ${config.dbPath}`);
  }
  db = await open({ filename: config.dbPath, driver: sqlite3.Database });
  await db.exec(`
    CREATE TABLE IF NOT EXISTS sessions (
      id TEXT PRIMARY KEY,
      timestamp TEXT,
      sessionStart INTEGER
    );
    CREATE TABLE IF NOT EXISTS engagement (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      sessionId TEXT,
      itemId TEXT,
      views INTEGER,
      lastViewedAt INTEGER,
      totalWatchMs INTEGER,
      completions INTEGER,
      FOREIGN KEY(sessionId) REFERENCES sessions(id)
    );
    CREATE TABLE IF NOT EXISTS perfLog (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      sessionId TEXT,
      timestamp TEXT,
      perfNow REAL,
      usedJSHeapSize INTEGER,
      totalJSHeapSize INTEGER,
      jsHeapSizeLimit INTEGER,
      action TEXT,
      batchId INTEGER,
      uploadMode TEXT,
      details TEXT,
      FOREIGN KEY(sessionId) REFERENCES sessions(id)
    );
    CREATE TABLE IF NOT EXISTS debug (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      sessionId TEXT,
      message TEXT,
      timestamp TEXT,
      FOREIGN KEY(sessionId) REFERENCES sessions(id)
    );
    CREATE TABLE IF NOT EXISTS sessionMetrics (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      sessionId TEXT,
      data TEXT,
      FOREIGN KEY(sessionId) REFERENCES sessions(id)
    );
  `);
  return db;
}
// --- Utility: Accept and log all valid data, never reject whole request ---
function logInvalid(type: string, data: unknown, error: unknown) {
  logger.info(
    `[metrics] Invalid ${type}: ${JSON.stringify(error)} | Data: ${JSON.stringify(data)}`,
  );
}
// --- Session Helper ---
async function ensureSessionExists(
  database: Database,
  sessionId: string,
  timestamp?: string,
  sessionStart?: number,
) {
  // Try to find session by id
  const sessionRow = await database.get('SELECT id FROM sessions WHERE id = ?', sessionId);
  if (!sessionRow) {
    await database.run(
      'INSERT INTO sessions (id, timestamp, sessionStart) VALUES (?, ?, ?)',
      sessionId,
      timestamp || new Date().toISOString(),
      sessionStart ?? null,
    );
    logger.info(`[metrics] Created minimal session for id=${sessionId}`);
  }
}
export async function saveMetrics(
  payload: unknown,
  clientSessionId?: string,
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  _env?: unknown,
) {
  const database = await initDb();
  // PerfLog batch
  if (Array.isArray(payload)) {
    for (const entry of payload) {
      const result = PerfLogEntrySchema.safeParse(entry);
      if (!result.success) {
        logInvalid('perfLog', entry, result.error.format());
        continue;
      }
      // Resolve sessionId
      const resolvedSessionId = result.data.sessionId || clientSessionId;
      if (!resolvedSessionId) {
        logger.warn('[metrics] No sessionId found for perfLog entry. Skipping.');
        continue;
      }
      await ensureSessionExists(database, resolvedSessionId, result.data.timestamp);
      const { timestamp, perfNow, memory, action, batchId, uploadMode, ...rest } = result.data;
      const details = JSON.stringify({ ...rest });
      await database.run(
        `INSERT INTO perfLog (sessionId, timestamp, perfNow, usedJSHeapSize, totalJSHeapSize, jsHeapSizeLimit, action, batchId, uploadMode, details)
         VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`,
        resolvedSessionId,
        timestamp,
        perfNow,
        memory?.usedJSHeapSize ?? -1,
        memory?.totalJSHeapSize ?? -1,
        memory?.jsHeapSizeLimit ?? -1,
        action,
        batchId ?? null,
        uploadMode ?? null,
        details,
      );
    }
    return;
  }
  // Full payload
  const result = FullPayloadSchema.safeParse(payload);
  if (!result.success) {
    logInvalid('fullPayload', payload, result.error.format());
    // Try to salvage valid subfields
    if (payload && typeof payload === 'object') {
      // Try perfLog
      const perfLogArray = Array.isArray((payload as Record<string, unknown>).perfLog)
        ? ((payload as Record<string, unknown>).perfLog as unknown[])
        : [];
      for (const entry of perfLogArray) {
        const perfResult = PerfLogEntrySchema.safeParse(entry);
        if (perfResult.success) {
          const { timestamp, perfNow, memory, action, batchId, uploadMode, ...rest } =
            perfResult.data;
          const details = JSON.stringify({ ...rest });
          await database.run(
            `INSERT INTO perfLog (sessionId, timestamp, perfNow, usedJSHeapSize, totalJSHeapSize, jsHeapSizeLimit, action, batchId, uploadMode, details)
             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`,
            null,
            timestamp,
            perfNow,
            memory?.usedJSHeapSize ?? -1,
            memory?.totalJSHeapSize ?? -1,
            memory?.jsHeapSizeLimit ?? -1,
            action,
            batchId ?? null,
            uploadMode ?? null,
            details,
          );
        } else {
          logInvalid('perfLog', entry, perfResult.error.format());
        }
      }
      // Try debug
      const debugArray = Array.isArray((payload as Record<string, unknown>).debug)
        ? ((payload as Record<string, unknown>).debug as unknown[])
        : [];
      for (const entry of debugArray) {
        const debugResult = DebugLogSchema.safeParse(entry);
        if (debugResult.success) {
          await database.run(
            `INSERT INTO debug (sessionId, message, timestamp) VALUES (?, ?, ?)`,
            null,
            debugResult.data.message,
            debugResult.data.timestamp,
          );
        } else {
          logInvalid('debug', entry, debugResult.error.format());
        }
      }
      // Try engagement
      const engagementObj = (payload as Record<string, unknown>).engagement;
      if (engagementObj && typeof engagementObj === 'object' && !Array.isArray(engagementObj)) {
        for (const [itemId, stats] of Object.entries(engagementObj)) {
          const statsResult = EngagementStatsSchema.safeParse(stats);
          if (statsResult.success) {
            await database.run(
              `INSERT INTO engagement (sessionId, itemId, views, lastViewedAt, totalWatchMs, completions) VALUES (?, ?, ?, ?, ?, ?)`,
              null,
              itemId,
              statsResult.data.views,
              statsResult.data.lastViewedAt,
              statsResult.data.totalWatchMs,
              statsResult.data.completions,
            );
          } else {
            logInvalid('engagement', stats, statsResult.error.format());
          }
        }
      }
    }
    return;
  }
  const {
    engagement,
    perfLog,
    debug,
    timestamp,
    sessionStart,
    sessionMetrics: sessionMetricsFromPayload,
  } = result.data;
  // Resolve sessionId
  const resolvedSessionId = sessionMetricsFromPayload.sessionId || clientSessionId;
  await ensureSessionExists(
    database,
    resolvedSessionId || 'unknown-session',
    timestamp,
    sessionStart,
  );
  // Insert engagement
  for (const [itemId, stats] of Object.entries(engagement)) {
    await database.run(
      `INSERT INTO engagement (sessionId, itemId, views, lastViewedAt, totalWatchMs, completions) VALUES (?, ?, ?, ?, ?, ?)`,
      resolvedSessionId,
      itemId,
      stats.views,
      stats.lastViewedAt,
      stats.totalWatchMs,
      stats.completions,
    );
  }
  // Insert perfLog
  for (const entry of perfLog) {
    const { timestamp, perfNow, memory, action, batchId, uploadMode, ...rest } = entry;
    const details = JSON.stringify({ ...rest });
    await database.run(
      `INSERT INTO perfLog (sessionId, timestamp, perfNow, usedJSHeapSize, totalJSHeapSize, jsHeapSizeLimit, action, batchId, uploadMode, details) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`,
      resolvedSessionId,
      timestamp,
      perfNow,
      memory?.usedJSHeapSize ?? -1,
      memory?.totalJSHeapSize ?? -1,
      memory?.jsHeapSizeLimit ?? -1,
      action,
      batchId ?? null,
      uploadMode ?? null,
      details,
    );
  }
  // Insert debug
  for (const entry of debug) {
    await database.run(
      `INSERT INTO debug (sessionId, message, timestamp) VALUES (?, ?, ?)`,
      resolvedSessionId,
      entry.message,
      entry.timestamp,
    );
  }
  // Insert sessionMetrics
  await database.run(
    `INSERT INTO sessionMetrics (sessionId, data) VALUES (?, ?)`,
    resolvedSessionId,
    JSON.stringify(sessionMetricsFromPayload),
  );
}
// Graceful shutdown
process.on('SIGINT', async () => {
  if (db) await db.close();
  logger.info('SQLite connection closed (SIGINT)');
  process.exit(0);
});
process.on('SIGTERM', async () => {
  if (db) await db.close();
  logger.info('SQLite connection closed (SIGTERM)');
  process.exit(0);
});
</file>

<file path="modules/app-metrics/index.ts">
export * as appGalleryGenerator from './app_gallery-generator';
</file>

<file path="modules/app-metrics/README.md">
# App Metrics Module

This module provides a structure for collecting and exposing metrics for different applications within the Central Hub project.

## Structure

- Each app has its own subfolder under `app-metrics/` (e.g., `app_gallery-generator/`).
- Each subfolder contains its own controller, service, and index.ts for modularity.
- The main `index.ts` re-exports all app metrics modules.

## Adding Metrics for a New App

1. **Create a new folder:**
   - Example: `src/modules/app-metrics/my_new_app/`
2. **Add your controller and service:**
   - `metricsController.ts` and `metricsService.ts`
3. **Export from `index.ts`:**
   - `export * from './metricsController';`
   - `export * from './metricsService';`
4. **Register the endpoint:**

   - In `src/routes/metrics.routes.ts`, add:

     ```typescript
     import { myNewAppMetricsController } from '../modules/app-metrics/my_new_app/metricsController';
     if (config.features.metrics) {
       router.post('/api/metrics/my-new-app', myNewAppMetricsController.handleMetrics);
     }
     ```

## Feature Toggling

- Metrics endpoints are only registered if `config.features.metrics` is enabled in `src/config/server.config.ts`.
- To disable all metrics endpoints, set `metrics: false` in the config.

## Example Folder Structure

```
src/modules/app-metrics/
  app_gallery-generator/
    metricsController.ts
    metricsService.ts
    index.ts
  my_new_app/
    metricsController.ts
    metricsService.ts
    index.ts
  index.ts
  README.md
```

## Best Practices

- Use RESTful route patterns: `/api/metrics/:app`.
- Keep each app’s metrics logic isolated in its own folder.
- Document new endpoints in the main README and provide usage examples.
</file>

<file path="modules/embeddings/embedding.service.ts">
/* eslint-disable @typescript-eslint/no-explicit-any */
/* eslint-disable @typescript-eslint/no-unused-vars */
// modules/embeddings/embedding.service.ts
import { spawn, ChildProcessWithoutNullStreams, execFile, execSync } from 'child_process'; // Import execSync
import Ajv, { ValidateFunction } from 'ajv';
import addFormats from 'ajv-formats';
import clipCacheSchema from '../../../schemas/clipCache.schema.json'; // Adjust path if needed
import path from 'path';
import fs from 'fs/promises'; // Use fs/promises for async file operations
import { promisify } from 'util'; // Needed for promisifying execFile if not using fs/promises directly
import { imageSize } from 'image-size'; // Import image-size correctly
import {
  Logger,
  ConsoleTransport,
  FileTransport,
  JsonFormatter,
  PrettyFormatter,
} from '../../utils/logger';
import { config } from '../../config/server.config'; // Assuming config for paths
// --- Types --- //
/**
 * Represents the structure of a single entry in the ClipCache.
 */
export interface ClipCacheEntry {
  schemaVersion: string;
  filePath: string;
  mediaType: 'image' | 'video';
  mtime: number;
  fileSize: number;
  dimensions: { width: number; height: number };
  duration: number | null; // Made explicitly number | null
  embedding: number[];
  embeddingModel: string;
  embeddingConfig: {
    numFrames?: number | null;
    augmentation?: boolean;
    samplingMethod?: string;
    [k: string]: unknown;
  };
  processingTimestamp: string; // ISO 8601 date-time string
  debugMetadata?: { [k: string]: unknown };
  error?: string;
  detail?: string;
}
/**
 * Represents the entire ClipCache structure (filePath -> ClipCacheEntry mapping).
 */
export type ClipCache = Record<string, ClipCacheEntry>;
// --- AJV Setup --- //
const ajv = new Ajv({ allErrors: true });
addFormats(ajv);
// Compile the schema for a SINGLE entry - validation is crucial
let validateEntry: ValidateFunction<ClipCacheEntry>;
try {
  // Ensure the definition path is correct within your schema file
  if (!clipCacheSchema.definitions || !(clipCacheSchema.definitions as any).ClipCacheEntry) {
    throw new Error(
      'Schema definitions or ClipCacheEntry definition missing in clipCache.schema.json',
    );
  }
  validateEntry = ajv.compile<ClipCacheEntry>((clipCacheSchema.definitions as any).ClipCacheEntry);
} catch (err: any) {
  console.error('FATAL: Failed to compile ClipCacheEntry JSON Schema:', err);
  // Depending on desired behavior, you might exit or use a dummy validator
  // Using a dummy validator that always fails ensures no invalid data passes:
  validateEntry = ((data: any) => {
    (validateEntry as any).errors = [{ message: 'Schema compilation failed' }];
    return false;
  }) as ValidateFunction<ClipCacheEntry>;
  // Alternatively, exit if schema validation is critical: process.exit(1);
}
// --- Logging Setup --- //
const NODE_LOG_PREFIX = '[NodeEmbeddingService]';
// NOTE: Consider moving log file path to config/server.config.ts
// Log file path now comes from config
const LOG_FILE_PATH = path.resolve(
  config.logging.logDir, // Use centralized log directory from config
  'embedding_service.log',
);
// Ensure log directory exists
try {
  fs.mkdir(path.dirname(LOG_FILE_PATH), { recursive: true });
} catch (e) {
  console.error('Error creating log directory:', e);
}
const logger = new Logger({
  transports: [
    new ConsoleTransport({
      formatter: new PrettyFormatter({
        useColors: true,
        useBoxes: true,
        showTimestamp: true,
      }),
      level: config.logging.level || 'info', // Use level from config or default
    }),
    new FileTransport({
      filename: LOG_FILE_PATH,
      formatter: new JsonFormatter(),
      level: 'debug', // Keep file log level potentially more verbose
    }),
  ],
});
// Add a dedicated logger for failed validation attempts
const FAILED_VALIDATION_LOG_PATH = path.resolve(config.logging.logDir, 'failed_validation.log');
const failedValidationLogger = new Logger({
  transports: [
    new FileTransport({
      filename: FAILED_VALIDATION_LOG_PATH,
      formatter: new JsonFormatter(),
      level: 'error',
    }),
  ],
  level: 'error',
});
// --- Configuration --- //
// Read Python settings from the centralized config object
const PYTHON_EXECUTABLE = config.embedding.pythonExecutable;
const PYTHON_SCRIPT_PATH = config.embedding.pythonScriptPath;
const PYTHON_MODEL_ARGS: string[] = config.embedding?.modelArgs || [];
const INACTIVITY_TIMEOUT_MS = config.embedding?.inactivityTimeoutMs || 5 * 60 * 1000; // 5 minutes default
const PYTHON_SCRIPT_TIMEOUT_MS = config.embedding?.scriptTimeoutMs || 15 * 60 * 1000; // 15 minutes default
// --- Types (Internal) --- //
interface EmbeddingResponseFromPython {
  // Structure expected directly from the Python script's JSON output per file
  embedding?: number[];
  error?: string;
  detail?: string;
  debugMetadata?: any;
}
interface FileMetadata {
  mtime: number;
  fileSize: number;
  dimensions: { width: number; height: number };
  duration: number | null;
  mediaType: 'image' | 'video';
}
interface EmbeddingRequestInternal {
  paths: string[];
  resolve: (result: ClipCache) => void; // Resolve with ClipCache structure
  reject: (error: Error) => void;
  startTime: number; // Track start time for logging duration
  timeoutHandle?: NodeJS.Timeout; // Store timeout handle
}
// --- Status Types (Exported) --- //
export type EmbeddingServiceState = 'IDLE' | 'PROCESSING' | 'STARTING' | 'ERROR' | 'STOPPED';
export interface EmbeddingServiceStatus {
  state: EmbeddingServiceState;
  pid: number | null;
  isStarting: boolean;
  isProcessing: boolean;
  queueLength: number;
  currentBatch?: { count: number; total: number; current: string };
  lastError?: string; // Made optional
}
// Promisify execFile for ffprobe
const execFileAsync = promisify(execFile);
/**
 * Manages a persistent Python child process for CLIP embedding generation.
 * Handles spawning, communication (stdin/stdout), metadata fetching, validation,
 * and error handling/restarts.
 */
class EmbeddingService {
  private pythonProcess: ChildProcessWithoutNullStreams | null = null;
  private isStarting = false;
  private isStopping = false; // Flag to prevent restarts during manual stop
  private requestQueue: EmbeddingRequestInternal[] = [];
  private currentProcessing: EmbeddingRequestInternal | null = null;
  private responseBuffer = '';
  private inactivityTimer: NodeJS.Timeout | null = null;
  private lastProgress: { processed: number; total: number; current: string } | null = null;
  private lastError: string | null = null;
  constructor() {
    logger.info(`${NODE_LOG_PREFIX} Initializing Embedding Service...`);
    this.validateConfig();
    this.checkDependencies(); // Check for ffprobe on startup
    this.setupExitHandlers();
    // Do not start the Python process immediately; wait for the first request.
  }
  private validateConfig() {
    // Basic checks for essential config/paths
    if (!PYTHON_EXECUTABLE)
      logger.warn(`${NODE_LOG_PREFIX} PYTHON_EXECUTABLE not set, defaulting.`);
    try {
      fs.access(PYTHON_SCRIPT_PATH, fs.constants.R_OK); // Check if script is readable
    } catch (e) {
      logger.error(
        `${NODE_LOG_PREFIX} Python script not found or not readable at: ${PYTHON_SCRIPT_PATH}`,
      );
      this.lastError = `Python script not accessible at ${PYTHON_SCRIPT_PATH}`;
      // Consider preventing service start if script is missing
    }
  }
  private checkDependencies() {
    // Check for ffprobe
    try {
      execSync('ffprobe -version', { stdio: 'ignore' }); // Execute command, ignore output
      logger.info(`${NODE_LOG_PREFIX} Dependency check: ffprobe found.`);
    } catch (error) {
      logger.error(
        `${NODE_LOG_PREFIX} Dependency check failed: ffprobe not found in PATH. Video metadata extraction will fail.`,
      );
      // Consider setting an error state or warning prominently
    }
    // Add checks for ffmpeg if needed by python script logic too?
  }
  // --- Python Process Management --- //
  private resetInactivityTimer() {
    this.clearInactivityTimer();
    // Only set timer if process exists and we are not deliberately stopping it
    if (this.pythonProcess && !this.isStopping) {
      this.inactivityTimer = setTimeout(() => {
        this.stopDueToInactivity();
      }, INACTIVITY_TIMEOUT_MS);
      // Allow Node.js to exit if this timer is the only thing active
      if (this.inactivityTimer.unref) this.inactivityTimer.unref();
    }
  }
  private clearInactivityTimer() {
    if (this.inactivityTimer) {
      clearTimeout(this.inactivityTimer);
      this.inactivityTimer = null;
    }
  }
  private stopDueToInactivity() {
    if (this.isStopping) return; // Already stopping
    logger.info(`${NODE_LOG_PREFIX} Stopping Python process due to inactivity.`);
    this.isStopping = true; // Mark as stopping to prevent auto-restart
    this.stop(); // Use the main stop method
  }
  private async startPythonProcess(): Promise<void> {
    if (this.pythonProcess || this.isStarting) {
      logger.warn(`${NODE_LOG_PREFIX} Process already running or starting.`);
      return Promise.resolve(); // Don't reject, just return
    }
    if (this.lastError === `Python script not accessible at ${PYTHON_SCRIPT_PATH}`) {
      logger.error(`${NODE_LOG_PREFIX} Cannot start process, script is inaccessible.`);
      return Promise.reject(new Error(this.lastError));
    }
    this.isStarting = true;
    this.isStopping = false; // Reset stopping flag
    this.lastError = null; // Clear previous error
    logger.info(
      `${NODE_LOG_PREFIX} Starting Python process: ${PYTHON_EXECUTABLE} "${PYTHON_SCRIPT_PATH}" ${PYTHON_MODEL_ARGS.join(' ')}`,
    );
    return new Promise((resolve, reject) => {
      try {
        // Ensure script path is quoted if it contains spaces
        this.pythonProcess = spawn(PYTHON_EXECUTABLE, [PYTHON_SCRIPT_PATH, ...PYTHON_MODEL_ARGS], {
          stdio: ['pipe', 'pipe', 'pipe'], // stdin, stdout, stderr
        });
        this.isStarting = false; // Process spawned, not necessarily fully ready, but starting phase over
        this.pythonProcess.stdout.on('data', (data: Buffer) => {
          // Optimization: Decode buffer only once
          const chunk = data.toString('utf-8');
          logger.debug(`${NODE_LOG_PREFIX} [PYTHON STDOUT RAW] ${chunk.length} chars`);
          this.responseBuffer += chunk;
          this.processResponseBuffer(); // Process lines efficiently
        });
        this.pythonProcess.stderr.on('data', (data: Buffer) => {
          const lines = data.toString('utf-8').split('\n');
          lines.forEach((line) => {
            const trimmed = line.trim();
            if (!trimmed) return; // Skip empty lines
            if (trimmed.startsWith('PROGRESS:')) {
              try {
                const json = trimmed.substring(9).trim(); // More robust substring
                const progress = JSON.parse(json);
                if (progress && typeof progress === 'object') {
                  this.lastProgress = {
                    processed: Number(progress.processed) || 0,
                    total: Number(progress.total) || 0,
                    current: String(progress.current || ''),
                  };
                } else {
                  logger.warn(`${NODE_LOG_PREFIX} Invalid progress JSON structure: ${json}`);
                }
              } catch (e) {
                logger.warn(
                  `${NODE_LOG_PREFIX} Failed to parse progress line: "${trimmed}", Error: ${(e as Error).message}`,
                );
              }
            } else {
              // Log other stderr lines as errors from Python script
              logger.error(`${NODE_LOG_PREFIX} [PYTHON STDERR] ${trimmed}`);
            }
          });
        });
        this.pythonProcess.on('error', (err) => {
          logger.error(`${NODE_LOG_PREFIX} Python process spawn error: ${err.message}`);
          this.lastError = err.message;
          const startError = new Error(`Python process failed to spawn: ${err.message}`);
          this.handleProcessExit(startError); // Pass error for rejection
          reject(startError); // Reject the start promise
        });
        this.pythonProcess.on('exit', (code, signal) => {
          const exitMsg = `Python process exited (Code: ${code}, Signal: ${signal})`;
          logger.warn(`${NODE_LOG_PREFIX} ${exitMsg}`);
          // Only set lastError if it exited unexpectedly (non-zero code, or signal)
          if (code !== 0 || signal) {
            this.lastError = exitMsg;
          }
          this.handleProcessExit(new Error(exitMsg)); // Pass error for rejection
          // Do not reject the start promise here if it already resolved
        });
        logger.info(`${NODE_LOG_PREFIX} Python process started (PID: ${this.pythonProcess.pid}).`);
        this.resetInactivityTimer(); // Start tracking activity
        this.processQueue(); // Process any queued requests
        resolve(); // Resolve the start promise
      } catch (error: any) {
        logger.error(`${NODE_LOG_PREFIX} Failed to spawn Python process: ${error.message}`);
        this.isStarting = false;
        this.pythonProcess = null;
        const spawnError = new Error(`Failed to spawn Python process: ${error.message}`);
        this.lastError = spawnError.message;
        this.rejectQueue(spawnError); // Reject queued items
        reject(spawnError); // Reject the start promise
      }
    });
  }
  /** Efficiently process the response buffer line by line */
  private processResponseBuffer() {
    let newlineIndex;
    // Use a loop for efficiency if multiple lines arrive in one chunk
    while ((newlineIndex = this.responseBuffer.indexOf('\n')) >= 0) {
      const jsonResponse = this.responseBuffer.substring(0, newlineIndex).trim();
      // Advance the buffer past the processed line and newline character
      this.responseBuffer = this.responseBuffer.substring(newlineIndex + 1);
      if (jsonResponse) {
        logger.debug(
          `${NODE_LOG_PREFIX} [PYTHON RESPONSE] Processing response line (${jsonResponse.length} chars)`,
        );
        this.handlePythonJsonResponse(jsonResponse); // Handle the parsed line
      }
    }
  }
  private handleProcessExit(error?: Error): void {
    const pid = this.pythonProcess?.pid;
    logger.debug(`${NODE_LOG_PREFIX} handleProcessExit called (PID: ${pid})`);
    this.pythonProcess = null; // Mark process as gone
    this.isStarting = false; // Ensure starting flag is reset
    this.clearInactivityTimer();
    // If there was an active request, reject it
    if (this.currentProcessing) {
      const exitError = error || new Error('Python embedding process exited unexpectedly.');
      logger.error(
        `${NODE_LOG_PREFIX} Python process exited while processing request for ${this.currentProcessing.paths.length} paths.`,
      );
      this.currentProcessing.reject(exitError);
      // Clear timeout associated with this request
      if (this.currentProcessing.timeoutHandle) clearTimeout(this.currentProcessing.timeoutHandle);
      this.currentProcessing = null;
    }
    // Reject all remaining queued requests
    const queueError = error || new Error('Python embedding process is not available.');
    this.rejectQueue(queueError);
    // Conditionally restart if not manually stopped
    if (!this.isStopping) {
      logger.info(`${NODE_LOG_PREFIX} Attempting to restart Python process in 5 seconds...`);
      // Use setTimeout directly, no need for async/await here
      setTimeout(() => {
        logger.debug(`${NODE_LOG_PREFIX} Restart timer fired.`);
        this.startPythonProcess().catch((err) => {
          logger.error(`${NODE_LOG_PREFIX} Auto-restart failed: ${err.message}`);
          // Keep lastError updated if restart fails
          this.lastError = `Auto-restart failed: ${err.message}`;
        });
      }, 5000);
    } else {
      logger.info(
        `${NODE_LOG_PREFIX} Manual stop initiated, Python process will not be restarted.`,
      );
      this.isStopping = false; // Reset flag after handling exit during stop
    }
  }
  // --- Metadata Fetching --- //
  /** Fetches metadata for a single file. */
  private async getFileMetadata(filePath: string): Promise<FileMetadata> {
    let mtime = 0; // Default to 0 for consistency if stat fails
    let fileSize = 0;
    let dimensions = { width: 1, height: 1 }; // Default dimension
    let duration: number | null = null;
    let mediaType: 'image' | 'video' = 'image'; // Default assumption
    try {
      const ext = path.extname(filePath).toLowerCase();
      // Basic media type detection based on extension
      mediaType = ['.mp4', '.mov', '.webm', '.avi', '.mkv', '.wmv', '.m4v'].includes(ext)
        ? 'video'
        : 'image';
      // 1. Get file stats (mtime, size)
      const stat = await fs.stat(filePath);
      mtime = stat.mtimeMs;
      fileSize = stat.size;
      // 2. Get dimensions (and duration for videos)
      if (mediaType === 'image') {
        try {
          // Optimization: Read only necessary bytes for image-size
          const buffer = Buffer.alloc(1024); // Adjust size if needed for specific formats
          const fd = await fs.open(filePath, 'r');
          await fd.read(buffer, 0, 1024, 0);
          await fd.close();
          const dim = imageSize(buffer); // Pass buffer
          dimensions = { width: dim?.width ?? 1, height: dim?.height ?? 1 };
        } catch (imgErr) {
          logger.warn(
            `${NODE_LOG_PREFIX} Failed to get image dimensions for ${filePath}: ${(imgErr as Error).message}. Using default 1x1.`,
          );
          // Keep default dimensions
        }
      } else if (mediaType === 'video') {
        try {
          // Ensure ffprobe path is correct or in system PATH
          const { stdout } = await execFileAsync('ffprobe', [
            '-v',
            'error',
            '-select_streams',
            'v:0', // Select video stream 0
            '-show_entries',
            'stream=width,height,duration',
            '-of',
            'json', // Output as JSON
            filePath,
          ]);
          const info = JSON.parse(stdout);
          if (info.streams && info.streams[0]) {
            const s = info.streams[0];
            dimensions = { width: s.width ?? 1, height: s.height ?? 1 };
            duration = s.duration && !isNaN(parseFloat(s.duration)) ? parseFloat(s.duration) : null;
          } else {
            logger.warn(`${NODE_LOG_PREFIX} ffprobe found no video stream info for ${filePath}.`);
          }
        } catch (ffprobeErr) {
          logger.warn(
            `${NODE_LOG_PREFIX} ffprobe failed for ${filePath}: ${(ffprobeErr as Error).message}.`,
          );
          // Keep default dimensions/duration
        }
      }
    } catch (statErr) {
      logger.warn(
        `${NODE_LOG_PREFIX} Failed to stat file ${filePath}: ${(statErr as Error).message}. Using default metadata.`,
      );
      // Keep default mtime/size if stat fails, might indicate file removed
      // Should we propagate this error more clearly?
    }
    return { mtime, fileSize, dimensions, duration, mediaType };
  }
  /** Fetches metadata for multiple files concurrently. */
  private async getBatchMetadata(filePaths: string[]): Promise<Record<string, FileMetadata>> {
    const metadataPromises = filePaths.map((fp) => this.getFileMetadata(fp));
    const results = await Promise.allSettled(metadataPromises);
    const metadataMap: Record<string, FileMetadata> = {};
    results.forEach((result, index) => {
      const filePath = filePaths[index];
      if (result.status === 'fulfilled') {
        metadataMap[filePath] = result.value;
      } else {
        // Log error but still provide a default entry so processing can continue
        logger.error(
          `${NODE_LOG_PREFIX} Failed to get metadata for ${filePath} in batch: ${result.reason?.message || result.reason}`,
        );
        metadataMap[filePath] = {
          // Provide default/fallback metadata
          mtime: 0,
          fileSize: 0,
          dimensions: { width: 1, height: 1 },
          duration: null,
          mediaType: path.extname(filePath).match(/\.(mp4|mov|webm)$/i) ? 'video' : 'image', // Best guess
        };
      }
    });
    return metadataMap;
  }
  // --- Response Handling & Validation --- //
  private async handlePythonJsonResponse(jsonResponse: string): Promise<void> {
    if (!this.currentProcessing) {
      logger.warn(`${NODE_LOG_PREFIX} Received response from Python but no request is processing.`);
      return;
    }
    const requestStartTime = this.currentProcessing.startTime;
    const currentRequest = this.currentProcessing; // Capture ref in case it changes
    this.currentProcessing = null; // Mark as done processing *before* async metadata fetching
    try {
      const pythonOutput: Record<string, EmbeddingResponseFromPython> = JSON.parse(jsonResponse);
      const filePathsInResponse = Object.keys(pythonOutput);
      logger.debug(
        `${NODE_LOG_PREFIX} Parsed Python response for ${filePathsInResponse.length} files.`,
      );
      // --- Optimization: Fetch metadata concurrently for all files in the batch ---
      logger.debug(
        `${NODE_LOG_PREFIX} Fetching metadata for ${filePathsInResponse.length} files...`,
      );
      const batchMetadata = await this.getBatchMetadata(filePathsInResponse);
      logger.debug(`${NODE_LOG_PREFIX} Finished fetching metadata.`);
      const finalResults: ClipCache = {}; // Build the response object conforming to ClipCache
      for (const filePath of filePathsInResponse) {
        const pyEntry = pythonOutput[filePath];
        const meta = batchMetadata[filePath]; // Get pre-fetched metadata
        if (!meta) {
          logger.error(
            `${NODE_LOG_PREFIX} Metadata missing for ${filePath} after batch fetch. Skipping.`,
          );
          // Create an error entry?
          finalResults[filePath] = {
            schemaVersion: '1.0.0',
            filePath: filePath,
            error: 'Metadata fetch failed',
            // Add other required fields with defaults if possible, or make them optional in schema
            mtime: 0,
            fileSize: 0,
            dimensions: { width: 1, height: 1 },
            duration: null,
            mediaType: 'image',
            embedding: [],
            embeddingModel: 'unknown',
            embeddingConfig: {},
            processingTimestamp: new Date().toISOString(),
          } as ClipCacheEntry; // May fail validation if embedding is required
          continue;
        }
        // Extract model/config from debugMetadata if present
        const debug = pyEntry.debugMetadata || {};
        const embeddingModel = String(debug.model || config.embedding?.defaultModel || 'unknown');
        const embeddingConfig: ClipCacheEntry['embeddingConfig'] = {
          augmentation:
            typeof debug.enable_augmentation === 'boolean' ? debug.enable_augmentation : undefined,
          numFrames: typeof debug.num_frames === 'number' ? debug.num_frames : null,
          samplingMethod: typeof debug.method_used === 'string' ? debug.method_used : undefined,
        };
        // Construct the entry attempting to match the schema
        const entryData: Partial<ClipCacheEntry> = {
          schemaVersion: '1.0.0',
          filePath: filePath,
          embedding: pyEntry.embedding, // Will be validated later
          debugMetadata: pyEntry.debugMetadata,
          error: pyEntry.error != null ? String(pyEntry.error) : undefined,
          detail: pyEntry.detail != null ? String(pyEntry.detail) : undefined,
          processingTimestamp: new Date().toISOString(),
          mtime: meta.mtime,
          fileSize: meta.fileSize,
          dimensions: meta.dimensions,
          mediaType: meta.mediaType,
          duration: meta.duration,
          embeddingModel,
          embeddingConfig,
        };
        // Handle case where embedding failed in Python
        if (entryData.error && !entryData.embedding) {
          // Schema requires embedding. Set to empty array to pass validation,
          // client should check for error field.
          entryData.embedding = [];
        } else if (!entryData.embedding && !entryData.error) {
          // No embedding and no error? Treat as error.
          entryData.error = 'Embedding missing without error from Python';
          entryData.embedding = [];
        }
        // ---> VALIDATE the constructed entry against the schema <---
        if (validateEntry(entryData)) {
          // If valid, assign the validated (and now typed) entry
          finalResults[filePath] = entryData as ClipCacheEntry;
        } else {
          // If invalid, log details and store an error-focused object
          const validationErrors = JSON.stringify(validateEntry.errors);
          logger.error(
            `${NODE_LOG_PREFIX} Constructed cache entry failed validation for: ${filePath}`,
            {
              constructedData: entryData, // Log data before validation
              pythonData: pyEntry, // Log raw python data
              errors: validationErrors,
            },
          );
          // Also log to failed_validation.log
          failedValidationLogger.error(`Failed validation for: ${filePath}`, {
            constructedData: entryData,
            pythonData: pyEntry,
            errors: validationErrors,
          });
          // Create a minimal structure indicating validation failure
          // This structure *must* still pass basic validation if possible,
          // or the client needs specific handling for these error objects.
          finalResults[filePath] = {
            schemaVersion: '1.0.0',
            filePath: filePath,
            error: 'Internal schema validation failed',
            detail: validationErrors,
            // Add required fields with placeholder/default values
            mtime: meta.mtime, // Use fetched meta even on validation error
            fileSize: meta.fileSize,
            dimensions: meta.dimensions,
            duration: meta.duration,
            mediaType: meta.mediaType,
            embedding: [], // Empty embedding on validation error
            embeddingModel: embeddingModel,
            embeddingConfig: embeddingConfig,
            processingTimestamp: entryData.processingTimestamp || new Date().toISOString(),
          } as ClipCacheEntry; // Note: This might still fail if required fields missing
        }
      }
      // Resolve the original promise with the processed results
      const duration = Date.now() - requestStartTime;
      logger.info(
        `${NODE_LOG_PREFIX} Successfully processed batch of ${filePathsInResponse.length} paths in ${duration} ms.`,
      );
      currentRequest.resolve(finalResults);
      if (currentRequest.timeoutHandle) clearTimeout(currentRequest.timeoutHandle); // Clear timeout on success
    } catch (e: any) {
      logger.error(
        `${NODE_LOG_PREFIX} Failed to parse/process JSON response from Python: ${e.message}. Response: ${jsonResponse}`,
        e,
      );
      const processingError = new Error(`Failed to process response from Python: ${e.message}`);
      currentRequest.reject(processingError); // Reject the original promise
      if (currentRequest.timeoutHandle) clearTimeout(currentRequest.timeoutHandle); // Clear timeout on error
      this.lastError = processingError.message; // Update last error
    } finally {
      // Ensure we attempt to process the queue regardless of success/failure of this batch
      this.processQueue();
      this.resetInactivityTimer(); // Reset timer after processing a response
    }
  }
  // --- Request Queuing and Processing --- //
  private rejectQueue(error: Error): void {
    if (this.requestQueue.length > 0) {
      logger.warn(
        `${NODE_LOG_PREFIX} Rejecting ${this.requestQueue.length} queued request(s) due to error: ${error.message}`,
      );
      this.requestQueue.forEach((req) => {
        if (req.timeoutHandle) clearTimeout(req.timeoutHandle); // Clear individual timeouts
        req.reject(error);
      });
      this.requestQueue = []; // Clear the queue
    }
  }
  private processQueue(): void {
    if (
      this.currentProcessing ||
      this.requestQueue.length === 0 ||
      !this.pythonProcess ||
      this.isStarting
    ) {
      logger.debug(
        `${NODE_LOG_PREFIX} Skipping processQueue (Processing: ${!!this.currentProcessing}, Queue: ${this.requestQueue.length}, Proc: ${!!this.pythonProcess}, Starting: ${this.isStarting})`,
      );
      return; // Process busy, queue empty, or process not ready/starting
    }
    this.currentProcessing = this.requestQueue.shift()!; // Get next request from queue
    this.lastProgress = null; // Reset progress for new batch
    logger.info(
      `${NODE_LOG_PREFIX} Sending batch of ${this.currentProcessing.paths.length} paths to Python (Queue: ${this.requestQueue.length}).`,
    );
    const requestPayload = { imagePaths: this.currentProcessing.paths }; // Python script expects this structure
    try {
      // Add newline delimiter for Python script's readline()
      const requestJson = JSON.stringify(requestPayload) + '\n';
      this.responseBuffer = ''; // Clear buffer before sending new request
      // Handle potential write errors (e.g., process died between check and write)
      if (!this.pythonProcess?.stdin?.writable) {
        throw new Error('Python process stdin is not writable.');
      }
      this.pythonProcess.stdin.write(requestJson, (err) => {
        if (err) {
          logger.error(`${NODE_LOG_PREFIX} Failed to write to Python stdin: ${err.message}`);
          // Process might be dead, trigger exit handling
          const writeError = new Error(`Failed to send data to Python: ${err.message}`);
          this.currentProcessing?.reject(writeError);
          if (this.currentProcessing?.timeoutHandle)
            clearTimeout(this.currentProcessing.timeoutHandle);
          this.currentProcessing = null;
          // Don't necessarily kill here, let exit handler manage potential restart
          this.handleProcessExit(writeError);
        } else {
          logger.debug(`${NODE_LOG_PREFIX} Data written to Python stdin successfully.`);
          this.resetInactivityTimer(); // Reset timer after successful write
        }
      });
    } catch (error: any) {
      logger.error(`${NODE_LOG_PREFIX} Error writing to Python stdin: ${error.message}`);
      const catchError = new Error(`Error sending data to Python: ${error.message}`);
      this.currentProcessing.reject(catchError);
      if (this.currentProcessing.timeoutHandle) clearTimeout(this.currentProcessing.timeoutHandle);
      this.currentProcessing = null;
      // Trigger exit handling if write fails critically
      this.handleProcessExit(catchError);
    }
  }
  /**
   * Public method to request embeddings. Starts Python process if needed.
   */
  public async getEmbeddings(
    imagePaths: string[],
    timeoutMs = PYTHON_SCRIPT_TIMEOUT_MS,
  ): Promise<ClipCache> {
    // Return ClipCache structure
    // Start process if it's not running and not already stopping/starting
    if (!this.pythonProcess && !this.isStarting && !this.isStopping) {
      logger.info(`${NODE_LOG_PREFIX} Python process not running. Starting for new request...`);
      try {
        await this.startPythonProcess();
      } catch (startErr: any) {
        logger.error(
          `${NODE_LOG_PREFIX} Failed to start Python process for request: ${startErr.message}`,
        );
        // Reject immediately if start failed
        return Promise.reject(new Error(`Failed to start Python process: ${startErr.message}`));
      }
    } else {
      // If process exists, reset inactivity timer as a request is coming in
      this.resetInactivityTimer();
    }
    this.lastError = null; // Clear last error on new request attempt
    return new Promise((resolve, reject) => {
      let settled = false;
      const startTime = Date.now();
      const request: EmbeddingRequestInternal = {
        paths: imagePaths,
        startTime: startTime,
        resolve: (result: ClipCache) => {
          // Expect ClipCache
          if (settled) return;
          settled = true;
          if (request.timeoutHandle) clearTimeout(request.timeoutHandle);
          this.resetInactivityTimer(); // Reset timer on successful completion
          resolve(result);
        },
        reject: (error: Error) => {
          if (settled) return;
          settled = true;
          if (request.timeoutHandle) clearTimeout(request.timeoutHandle);
          logger.error(
            `${NODE_LOG_PREFIX} Request failed after ${Date.now() - startTime} ms: ${error.message}`,
          );
          reject(error);
        },
      };
      // Setup timeout for *this specific request*
      request.timeoutHandle = setTimeout(() => {
        if (settled) return;
        logger.warn(
          `${NODE_LOG_PREFIX} Request timed out after ${timeoutMs} ms for ${imagePaths.length} paths.`,
        );
        // Remove request from queue *if it's still there*
        const index = this.requestQueue.findIndex((r) => r === request);
        if (index > -1) {
          this.requestQueue.splice(index, 1);
          logger.debug(`${NODE_LOG_PREFIX} Removed timed-out request from queue.`);
        } else if (this.currentProcessing === request) {
          // If it was actively processing, we can't easily abort Python,
          // but we should reject the promise and nullify currentProcessing
          // so the queue can potentially continue. Maybe kill python? Risky.
          logger.error(
            `${NODE_LOG_PREFIX} Request timed out while actively processing. Python process might be stuck.`,
          );
          this.currentProcessing = null; // Allow queue to proceed, previous request is lost
          this.lastError = `Request timed out while processing ${imagePaths.length} paths.`;
          // Consider killing and restarting python process here if it's stuck
          this.stop(); // Force stop and restart cycle
        }
        request.reject(new Error(`Embedding request timed out after ${timeoutMs} ms.`));
      }, timeoutMs);
      // Add to queue and attempt processing
      this.requestQueue.push(request);
      logger.debug(
        `${NODE_LOG_PREFIX} Queued request for ${imagePaths.length} paths. Queue size: ${this.requestQueue.length}`,
      );
      // Trigger queue processing immediately if possible
      if (!this.currentProcessing && this.pythonProcess && !this.isStarting) {
        this.processQueue();
      }
    });
  }
  // --- Service Control --- //
  /** Manually stops the Python process and rejects pending requests. */
  public stop(): void {
    logger.info(`${NODE_LOG_PREFIX} Manual stop requested.`);
    this.isStopping = true; // Prevent restarts during manual stop
    this.clearInactivityTimer(); // Stop inactivity timer
    if (this.pythonProcess) {
      logger.info(`${NODE_LOG_PREFIX} Killing Python process (PID: ${this.pythonProcess.pid})...`);
      this.pythonProcess.kill(); // Send SIGTERM
      this.pythonProcess = null; // Assume it will exit
    } else {
      logger.info(`${NODE_LOG_PREFIX} Python process already stopped.`);
    }
    // Reject current and queued requests
    const stopError = new Error('Embedding service is stopping.');
    if (this.currentProcessing) {
      this.currentProcessing.reject(stopError);
      if (this.currentProcessing.timeoutHandle) clearTimeout(this.currentProcessing.timeoutHandle);
      this.currentProcessing = null;
    }
    this.rejectQueue(stopError);
    // isStopping will be reset by handleProcessExit if it triggers,
    // or reset on next successful start
  }
  public getStatus(): EmbeddingServiceStatus {
    let state: EmbeddingServiceState = 'IDLE';
    if (this.isStopping)
      state = 'STOPPED'; // Explicitly stopped state
    else if (this.isStarting) state = 'STARTING';
    else if (!this.pythonProcess && this.lastError)
      state = 'ERROR'; // Error state if process down + error exists
    else if (!this.pythonProcess && !this.lastError)
      state = 'STOPPED'; // Stopped cleanly or hasn't started
    else if (this.currentProcessing) state = 'PROCESSING';
    // IDLE = process running but no current task
    const status: EmbeddingServiceStatus = {
      state,
      pid: this.pythonProcess?.pid ?? null,
      isStarting: this.isStarting,
      isProcessing: !!this.currentProcessing,
      queueLength: this.requestQueue.length,
      currentBatch: this.lastProgress
        ? {
            count: this.lastProgress.processed,
            total: this.lastProgress.total,
            current: this.lastProgress.current,
          }
        : undefined,
      lastError: this.lastError || undefined,
    };
    return status;
  }
  private setupExitHandlers() {
    // Graceful shutdown: Ensure Python process is killed when Node exits
    const handleExit = () => {
      logger.info(`${NODE_LOG_PREFIX} Node process exiting. Stopping Python process...`);
      this.isStopping = true; // Prevent restarts during shutdown
      this.stop();
    };
    process.on('exit', handleExit);
    // Handle Ctrl+C, kill, etc.
    process.on('SIGINT', () => {
      logger.info(`${NODE_LOG_PREFIX} Received SIGINT.`);
      handleExit();
      process.exit(0); // Exit Node process after cleanup attempt
    });
    process.on('SIGTERM', () => {
      logger.info(`${NODE_LOG_PREFIX} Received SIGTERM.`);
      handleExit();
      process.exit(0); // Exit Node process after cleanup attempt
    });
    process.on('uncaughtException', (err) => {
      logger.child([err.stack]).error(`${NODE_LOG_PREFIX} Uncaught Exception: ${err.message}`);
      // Optionally try to stop python before exiting
      handleExit();
      process.exit(1); // Exit with error code
    });
    process.on('unhandledRejection', (reason, promise) => {
      logger.error(`${NODE_LOG_PREFIX} Unhandled Rejection at: ${promise}, reason: ${reason}`);
      // Optionally try to stop python before exiting
      handleExit();
      process.exit(1); // Exit with error code
    });
  }
}
// Export a singleton instance
export const embeddingService = new EmbeddingService();
</file>

<file path="modules/embeddings/embeddings.handler.ts">
/* eslint-disable @typescript-eslint/no-explicit-any */
import { Socket } from 'net';
import { IncomingRequest } from '../../entities/http';
import { sendResponse } from '../../entities/sendResponse';
import {
  Logger,
  ConsoleTransport,
  FileTransport,
  JsonFormatter,
  PrettyFormatter,
} from '../../utils/logger';
import { embeddingService } from './embedding.service'; // Import the service
import path from 'path'; // Import path
import { config } from '../../config/server.config'; // Import config
const logger = new Logger({
  transports: [
    new ConsoleTransport({
      formatter: new PrettyFormatter(),
      level: 'info',
    }),
    new FileTransport({
      filename: 'logs/embeddings.log',
      formatter: new JsonFormatter(),
      level: 'debug',
    }),
  ],
  level: 'debug',
  exitOnError: false,
});
// Helper to summarize large arrays for logging
function summarizeArray(arr: any[]): string {
  if (!Array.isArray(arr)) return String(arr);
  const len = arr.length;
  if (len === 0) return '[]';
  const preview = arr.slice(0, 5).map((x) => Number(x).toFixed(4));
  let min = null,
    max = null;
  try {
    min = Math.min(...arr);
    max = Math.max(...arr);
  } catch {
    // empty catch to handle non-numeric arrays
  }
  return `[Array(len=${len}, min=${min}, max=${max}, preview=[${preview.join(', ')}]...)]`;
}
function summarizeObject(obj: any): any {
  if (Array.isArray(obj)) return summarizeArray(obj);
  if (obj && typeof obj === 'object') {
    const copy: any = Array.isArray(obj) ? [] : {};
    for (const key in obj) {
      if (Array.isArray(obj[key]) && obj[key].length > 20) {
        copy[key] = summarizeArray(obj[key]);
      } else if (typeof obj[key] === 'object' && obj[key] !== null) {
        copy[key] = summarizeObject(obj[key]);
      } else {
        copy[key] = obj[key];
      }
    }
    return copy;
  }
  return obj;
}
export const embeddingsController = {
  /**
   * Handles POST requests to /api/embeddings
   * Expects a JSON body: { "imagePaths": ["path1", "path2", ...] }
   * Returns JSON: { "path1": { "embedding": [...] }, "path2": { "embedding": [...] }, ... }
   */
  async handleEmbeddingsRequest(req: IncomingRequest, sock: Socket) {
    logger.info(
      `[EmbeddingsHandler] Received request: ` + JSON.stringify(summarizeObject(req.body)),
    );
    if (req.method !== 'POST') {
      return sendResponse(
        sock,
        405,
        { 'Content-Type': 'application/json' },
        JSON.stringify({ error: 'Method Not Allowed' }),
      );
    }
    if (!req.body || req.body.length === 0) {
      return sendResponse(
        sock,
        400,
        { 'Content-Type': 'application/json' },
        JSON.stringify({ error: 'Request body is empty' }),
      );
    }
    try {
      const body = JSON.parse(req.body.toString('utf-8'));
      const requestedPaths = body.imagePaths;
      if (!Array.isArray(requestedPaths) || requestedPaths.length === 0) {
        return sendResponse(
          sock,
          400,
          { 'Content-Type': 'application/json' },
          JSON.stringify({ error: 'No file paths provided.' }),
        );
      }
      // --- Security: Validate paths ---
      const mediaDir = path.resolve(config.mediaDir); // Get absolute configured media directory
      const imagePaths: string[] = []; // Store only validated paths
      for (const reqPath of requestedPaths) {
        if (typeof reqPath !== 'string') {
          logger.warn(`[EmbeddingsHandler] Invalid path type received: ${typeof reqPath}`);
          continue; // Skip non-string paths
        }
        const absoluteReqPath = path.resolve(mediaDir, reqPath);
        // Check for path traversal and ensure it's within the media directory
        if (!absoluteReqPath.startsWith(mediaDir) && !config.testMode) {
          logger.error(`[EmbeddingsHandler] Invalid path detected (outside media dir): ${reqPath}`);
          return sendResponse(
            sock,
            400,
            { 'Content-Type': 'application/json' },
            JSON.stringify({
              error: 'Invalid file path provided.',
              detail: `Path ${reqPath} is outside the allowed directory.`,
            }),
          );
        }
        imagePaths.push(absoluteReqPath); // Use the validated absolute path
      }
      if (imagePaths.length === 0) {
        logger.warn(`[EmbeddingsHandler] No valid paths remaining after validation.`);
        return sendResponse(
          sock,
          400,
          { 'Content-Type': 'application/json' },
          JSON.stringify({ error: 'No valid file paths provided.' }),
        );
      }
      // --- End Path Validation ---
      logger.info(
        `[EmbeddingsHandler] Requesting embeddings for ${imagePaths.length} validated paths.`,
      );
      const embeddingsResult = await embeddingService.getEmbeddings(imagePaths);
      logger.info(
        `[EmbeddingsHandler] Sending response for ${imagePaths.length} paths. Result summary: ` +
          JSON.stringify(summarizeObject(embeddingsResult)),
      );
      sendResponse(
        sock,
        200,
        { 'Content-Type': 'application/json' },
        JSON.stringify(embeddingsResult),
      );
    } catch (error: any) {
      logger.error(`[EmbeddingsHandler] Error processing embedding request: ${error.message}`);
      logger.error(`[EmbeddingsHandler] Error detail: ` + JSON.stringify(summarizeObject(error)));
      // Distinguish between JSON parsing errors and service errors
      const statusCode = error instanceof SyntaxError ? 400 : 500;
      const errorMessage =
        error instanceof SyntaxError
          ? 'Invalid JSON in request body.'
          : 'Failed to process embeddings.';
      sendResponse(
        sock,
        statusCode,
        { 'Content-Type': 'application/json' },
        JSON.stringify({ error: errorMessage, detail: error.message }),
      );
    }
  },
  /**
   * Handles POST requests to /api/embeddings/shutdown
   * Instructs the embedding service to stop gracefully.
   */
  async handleShutdownRequest(req: IncomingRequest, sock: Socket) {
    logger.info(`[EmbeddingsHandler] Received shutdown request`);
    if (req.method !== 'POST') {
      return sendResponse(
        sock,
        405,
        { 'Content-Type': 'application/json' },
        JSON.stringify({ error: 'Method Not Allowed' }),
      );
    }
    embeddingService.stop(); // Call stop
    logger.info(`[EmbeddingsHandler] Embedding service stop initiated.`);
    sendResponse(
      sock,
      200,
      { 'Content-Type': 'application/json' },
      JSON.stringify({ message: 'Embedding service shutdown initiated.' }),
    );
  },
  /**
   * Handles GET requests to /api/embeddings/status
   * Returns JSON: { state, isStarting, isProcessing, queueLength, currentBatch, error }
   */
  async handleStatusRequest(req: IncomingRequest, sock: Socket) {
    if (req.method !== 'GET') {
      return sendResponse(
        sock,
        405,
        { 'Content-Type': 'application/json' },
        JSON.stringify({ error: 'Method Not Allowed' }),
      );
    }
    const status = embeddingService.getStatus();
    sendResponse(sock, 200, { 'Content-Type': 'application/json' }, JSON.stringify(status));
  },
};
</file>

<file path="modules/file-hosting/fileHostingController.ts">
// src/modules/file-hosting/fileHostingController.ts
import { Socket } from 'net';
import { sendResponse } from '../../entities/sendResponse';
import { IncomingRequest } from '../../entities/http';
import { FileHostingService } from './fileHostingService';
import { getHeader, getQuery } from '../../utils/httpHelpers';
import { config } from '../../config/server.config';
import logger from '../../utils/logger';
import { getMimeType } from '../../utils/helpers';
import { Readable } from 'stream';
const fileSvc = new FileHostingService(config.mediaDir);
export const fileHostingController = {
  /** GET /file?file=filename – serves a file (supports Range) */
  async getFile(req: IncomingRequest, sock: Socket) {
    logger.info(`[getFile] url=${req.url} path=${req.path} query=${JSON.stringify(req.query)}`);
    const fileName = getQuery(req, 'file');
    if (!fileName) {
      sendResponse(
        sock,
        400,
        { 'Content-Type': 'text/plain' },
        'Missing required "file" query parameter.',
      );
      return;
    }
    try {
      const rangeHdr = getHeader(req, 'range');
      let stream: Readable;
      const fileStat = await fileSvc.stat(fileName);
      const size = fileStat.size;
      if (rangeHdr) {
        const m = /bytes=(\d*)-(\d*)/.exec(rangeHdr);
        if (!m) {
          sendResponse(sock, 416, { 'Content-Type': 'text/plain' }, '416 Range Not Satisfiable');
          sock.end();
          return;
        }
        const startStr = m[1];
        const endStr = m[2];
        let start: number;
        let end: number;
        if (startStr) {
          start = parseInt(startStr, 10);
          end = endStr ? parseInt(endStr, 10) : size - 1;
        } else {
          const suffix = parseInt(endStr, 10);
          start = size - suffix;
          end = size - 1;
        }
        if (start > end || start < 0 || end >= size) {
          sendResponse(sock, 416, { 'Content-Type': 'text/plain' }, '416 Range Not Satisfiable');
          sock.end();
          return;
        }
        stream = await fileSvc.readFile(fileName, { start, end });
        if (!stream) throw new Error('Stream is undefined');
        stream.on('error', (err: Error) => {
          logger.error(`[getFile] Stream error: ${err.message}`);
          sendResponse(sock, 500, { 'Content-Type': 'text/plain' }, 'Internal Server Error');
        });
        const len = end - start + 1;
        sendResponse(
          sock,
          206,
          {
            'Content-Type': getMimeType(fileName) || 'application/octet-stream',
            'Accept-Ranges': 'bytes',
            'Content-Range': `bytes ${start}-${end}/${size}`,
            'Content-Length': String(len),
          },
          stream,
        );
      } else {
        stream = await fileSvc.readFile(fileName);
        if (!stream) throw new Error('Stream is undefined');
        stream.on('error', (err: Error) => {
          logger.error(`[getFile] Stream error: ${err.message}`);
          sendResponse(sock, 500, { 'Content-Type': 'text/plain' }, 'Internal Server Error');
        });
        const mimeType = getMimeType(fileName);
        sendResponse(
          sock,
          200,
          { 'Content-Type': mimeType, 'Content-Length': String(size) },
          stream,
        );
      }
    } catch (err: unknown) {
      logger.error(`[getFile] fileName=${fileName}, error=${(err as Error).message}`);
      sendResponse(sock, 404, { 'Content-Type': 'text/plain' }, `File "${fileName}" not found.`);
    }
  },
  /** GET /files – returns JSON list of filenames */
  async listFiles(req: IncomingRequest, sock: Socket) {
    try {
      const files = await fileSvc.listFiles();
      sendResponse(sock, 200, { 'Content-Type': 'application/json' }, JSON.stringify(files));
    } catch (err: unknown) {
      logger.error(`listFiles: ${(err as Error).message}`);
      sendResponse(sock, 500, { 'Content-Type': 'text/plain' }, 'Server error');
    }
  },
  /** POST /file – upload a media file */
  async uploadFile(req: IncomingRequest, sock: Socket) {
    // Assume fileName is provided as a query param or header, and body is the file data
    const fileName = getQuery(req, 'file') || req.headers['x-filename'];
    if (!fileName) {
      sendResponse(sock, 400, { 'Content-Type': 'text/plain' }, 'Missing file name');
      return;
    }
    const mimeType = getMimeType(fileName);
    if (
      !mimeType.startsWith('image/') &&
      !mimeType.startsWith('video/') &&
      !mimeType.startsWith('audio/')
    ) {
      sendResponse(sock, 400, { 'Content-Type': 'text/plain' }, 'Only media files allowed');
      return;
    }
    if (!req.body) {
      sendResponse(sock, 400, { 'Content-Type': 'text/plain' }, 'No file data');
      return;
    }
    // Save file using FileHostingService
    const fileSvc = new FileHostingService(config.mediaDir);
    // Convert Buffer or string to async iterable
    async function* bufferToAsyncIterable(buffer: Buffer) {
      yield buffer;
    }
    const data = typeof req.body === 'string' ? Buffer.from(req.body) : req.body;
    await fileSvc.saveFile(fileName, bufferToAsyncIterable(data));
    sendResponse(sock, 200, { 'Content-Type': 'text/plain' }, 'Upload successful');
  },
  /** DELETE /file?file=filename – delete a media file */
  async deleteFile(req: IncomingRequest, sock: Socket) {
    const fileName = getQuery(req, 'file');
    if (!fileName) {
      sendResponse(sock, 400, { 'Content-Type': 'text/plain' }, 'Missing file name');
      return;
    }
    const fileSvc = new FileHostingService(config.mediaDir);
    const absPath = fileSvc['resolveSafe'](fileName);
    try {
      await import('fs/promises').then((fs) => fs.unlink(absPath));
      sendResponse(sock, 200, { 'Content-Type': 'text/plain' }, 'File deleted');
    } catch {
      sendResponse(
        sock,
        404,
        { 'Content-Type': 'text/plain' },
        'File not found or could not be deleted',
      );
    }
  },
};
</file>

<file path="modules/file-hosting/fileHostingService.ts">
import { createReadStream, createWriteStream } from 'fs';
import { stat, readdir, mkdir } from 'fs/promises';
import { resolve } from 'path';
import { Readable } from 'stream';
export class FileHostingService {
  constructor(private readonly rootDir: string) {}
  private resolveSafe(relPath: string): string {
    const abs = resolve(this.rootDir, relPath);
    if (!abs.startsWith(this.rootDir)) throw new Error('Path traversal attempt');
    return abs;
  }
  async listFiles(relDir = '.'): Promise<string[]> {
    return await readdir(this.resolveSafe(relDir));
  }
  async stat(relPath: string) {
    return await stat(this.resolveSafe(relPath));
  }
  async readFile(relPath: string, range?: { start: number; end: number }): Promise<Readable> {
    const abs = this.resolveSafe(relPath);
    return createReadStream(abs, range);
  }
  async saveFile(relPath: string, data: AsyncIterable<Buffer>): Promise<void> {
    const abs = this.resolveSafe(relPath);
    await mkdir(resolve(abs, '..'), { recursive: true });
    const ws = createWriteStream(abs);
    for await (const chunk of data) ws.write(chunk);
    await new Promise<void>((res, rej) => {
      ws.end(res);
      ws.on('error', rej);
    });
  }
}
</file>

<file path="modules/file-hosting/index.ts">
// src/modules/file-hosting/index.ts
export * from './fileHostingController';
export * from './fileHostingService';
</file>

<file path="modules/file-streaming/fileService.ts">
// file-streamer/fileService.ts
/**
 * @deprecated This file is now obsolete after moving to file-hosting module.
 * This file is deprecated and should be removed. All file management logic is now in file-hosting/fileHostingService.ts.
 */
export class FileService {
  constructor(private readonly rootDir: string) {
    throw new Error('FileService is deprecated. Use file-hosting module instead.');
  }
}
</file>

<file path="modules/file-streaming/fileStreamingController.ts">
import { Socket } from 'net';
import { sendResponse } from '../../entities/sendResponse';
import { IncomingRequest } from '../../entities/http';
import { getHeader, getQuery } from '../../utils/httpHelpers';
import { config } from '../../config/server.config';
import { Logger } from '../../utils/logger';
// Instantiate logger (default mock lacked info/error in tests)
const logger = new Logger();
import { getMimeType } from '../../utils/helpers';
import { Readable } from 'stream';
import { FileHostingService } from '../file-hosting/fileHostingService';
const fileSvc = new FileHostingService(config.mediaDir);
export const fileStreamingController = {
  /** GET /stream?file=video.mp4 – streams file (supports Range) */
  async handleStream(req: IncomingRequest, sock: Socket) {
    if (!config.testMode) {
      logger.info(
        `[handleStream] url=${req.url} path=${req.path} query=${JSON.stringify(req.query)}`,
      );
    }
    const fileName = getQuery(req, 'file');
    if (!fileName) {
      sendResponse(
        sock,
        400,
        { 'Content-Type': 'text/plain' },
        'Missing required "file" query parameter.',
      );
      return;
    }
    try {
      const rangeHdr = getHeader(req, 'range');
      let stream: Readable;
      const fileStat = await fileSvc.stat(fileName);
      const size = fileStat.size;
      if (rangeHdr) {
        const m = /bytes=(\d*)-(\d*)/.exec(rangeHdr);
        if (!m) {
          sendResponse(sock, 416, { 'Content-Type': 'text/plain' }, '416 Range Not Satisfiable');
          sock.end();
          return;
        }
        const startStr = m[1];
        const endStr = m[2];
        let start: number;
        let end: number;
        if (startStr) {
          start = parseInt(startStr, 10);
          end = endStr ? parseInt(endStr, 10) : size - 1;
        } else {
          const suffix = parseInt(endStr, 10);
          start = size - suffix;
          end = size - 1;
        }
        if (start > end || start < 0 || end >= size) {
          sendResponse(sock, 416, { 'Content-Type': 'text/plain' }, '416 Range Not Satisfiable');
          sock.end();
          return;
        }
        stream = await fileSvc.readFile(fileName, { start, end });
        if (!stream) throw new Error('Stream is undefined');
        const len = end - start + 1;
        sendResponse(
          sock,
          206,
          {
            'Content-Type': getMimeType(fileName) || 'application/octet-stream',
            'Accept-Ranges': 'bytes',
            'Content-Range': `bytes ${start}-${end}/${size}`,
            'Content-Length': String(len),
          },
          stream,
        );
      } else {
        stream = await fileSvc.readFile(fileName);
        if (!stream) throw new Error('Stream is undefined');
        const mimeType = getMimeType(fileName);
        sendResponse(
          sock,
          200,
          { 'Content-Type': mimeType, 'Content-Length': String(size) },
          stream,
        );
      }
    } catch (err) {
      if (!config.testMode) {
        logger.error(`[handleStream] fileName=${fileName}, error=${(err as Error).message}`);
      }
      sendResponse(sock, 404, { 'Content-Type': 'text/plain' }, `File "${fileName}" not found.`);
    }
  },
};
</file>

<file path="modules/file-streaming/index.ts">
export * from './fileService';
export * from './fileStreamingController';
</file>

<file path="routes/embeddings.routes.ts">
import router from '../core/router';
import { embeddingsController } from '../modules/embeddings/embeddings.handler';
// Placeholder: Add feature flag check if needed, similar to other routes
// if (config.features.embeddings) {
router.post('/api/embeddings', embeddingsController.handleEmbeddingsRequest);
router.post('/api/embeddings/shutdown', embeddingsController.handleShutdownRequest);
router.get('/api/embeddings/status', embeddingsController.handleStatusRequest);
</file>

<file path="routes/file-hosting.routes.ts">
import router from '../core/router';
import { fileHostingController } from '../modules/file-hosting';
// List all files
router.get('/api/files', fileHostingController.listFiles);
// Upload a file
router.post('/api/files', fileHostingController.uploadFile);
// Get a specific file
router.get('/api/files/:filename', fileHostingController.getFile);
// Delete a specific file
router.del('/api/files/:filename', fileHostingController.deleteFile);
</file>

<file path="routes/files.routes.ts">
// routes/files.routes.ts
// DEPRECATED: This file is now handled by file-hosting.routes.ts for all /api/files endpoints.
// Please use file-hosting.routes.ts instead.
import router from '../core/router';
import { fileHostingController } from '../modules/file-hosting';
router.get('/files', fileHostingController.listFiles);
</file>

<file path="routes/index.ts">
// routes/index.ts
import './stream.routes';
import './files.routes';
import './metrics.routes';
import './embeddings.routes';
import './file-hosting.routes';
import router from '../core/router';
import { sendResponse } from '../entities/sendResponse';
['/echo', '/ping'].forEach((path) =>
  router.any(path, async (req, sock) => {
    /**
     * Determines the message to be used based on the request path and body.
     *
     * - If the path is `/ping`, the message is set to `undefined`.
     * - Otherwise, it attempts to parse the request body as JSON and extract the `message` property.
     * - If the `message` property exists and is a string, it is returned.
     * - If parsing fails or the `message` property is not a string, a default message of `'Hello, world!'` is returned.
     *
     * @constant
     * @type {string | undefined}
     */
    const message =
      path === '/ping'
        ? undefined
        : (() => {
            try {
              const body = req.body ? JSON.parse(req.body.toString()) : {};
              return typeof body?.message === 'string' ? body.message : 'Hello, world!';
            } catch {
              return 'Hello, world!';
            }
          })();
    const res = message ? JSON.stringify({ message }) : '';
    sendResponse(
      sock,
      200,
      { 'Content-Type': 'application/json', 'Content-Length': Buffer.byteLength(res).toString() },
      res,
    );
  }),
);
export {}; // side-effect imports run immediately
</file>

<file path="routes/metrics.routes.ts">
// routes/metrics.routes.ts
import router from '../core/router';
import { metricsController } from '../modules/app-metrics/app_gallery-generator';
import { config } from '../config/server.config';
// eslint-disable-next-line @typescript-eslint/no-unused-vars
import { sendResponse as sendCustomResponse } from '../entities/sendResponse';
import { Readable } from 'stream';
import { Socket } from 'net';
/**
 * Accepts the JSON payload, validates, and writes into SQLite.
 */
// CORS middleware for all /api/metrics/* routes
router.use(async (req, sock, next) => {
  if (req.path.startsWith('/api/metrics/')) {
    const headers = {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET,POST,OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type, X-Session-Id',
    };
    if (req.method === 'OPTIONS') {
      // Preflight request
      sock.write(
        [
          'HTTP/1.1 204 No Content',
          ...Object.entries(headers).map(([k, v]) => `${k}: ${v}`),
          '',
          '',
        ].join('\r\n'),
      );
      sock.end();
      return;
    } else {
      // For normal requests, add CORS headers to response
      req.ctx = req.ctx || {};
      req.ctx.corsHeaders = headers;
    }
  }
  await next();
});
if (config.features.metrics) {
  router.post('/api/metrics/gallery-generator', async (req, sock) => {
    // Define the custom response sender *locally*
    // eslint-disable-next-line @typescript-eslint/no-unused-vars
    const sendCustomResponse = (
      sock: Socket,
      status: number,
      headers: Record<string, string>,
      body?: string | Buffer | Readable,
    ) => {
      // Access CORS headers from the request context (set by middleware)
      const corsHeaders = req.ctx && req.ctx.corsHeaders ? req.ctx.corsHeaders : {};
      sendCustomResponse(sock, status, { ...headers, ...corsHeaders }, body);
    };
    // Patch sendResponse for this request
    //req.sendResponse = sendResponse;
    await metricsController.handleMetrics(req, sock);
  });
}
</file>

<file path="routes/stream.routes.ts">
// routes/stream.routes.ts
import router from '../core/router';
import { fileStreamingController } from '../modules/file-streaming';
import { config } from '../config/server.config';
if (config.features.fileStreaming) {
  router.get('/api/stream', fileStreamingController.handleStream);
}
</file>

<file path="utils/helpers.ts">
import { extname } from 'path';
import { mimeTypes } from './mimeTypes';
export function getMimeType(fileName: string): string {
  const ext = extname(fileName).toLowerCase();
  return mimeTypes[ext] || 'application/octet-stream';
}
</file>

<file path="utils/httpHelpers.ts">
// Simple header accessor that falls back to legacy Record<string,string>
import { IncomingRequest } from '../entities/http';
/**
 * Case-insensitive lookup that handles multi-value headers.
 * @param req - The incoming request object.
 * @param name - The name of the header to retrieve.
 * @returns The value of the header, or undefined if not found.
 * @example
 * const contentType = getHeader(req, 'Content-Type');
 * const userAgent = getHeader(req, 'User-Agent');
 * const customHeader = getHeader(req, 'X-Custom-Header');
 */
export function getHeader(req: IncomingRequest, name: string): string | undefined {
  const key = name.toLowerCase();
  if (req.headersMap && req.headersMap.has(key)) {
    return req.headersMap.get(key)![0]; // first value
  }
  return req.headers?.[key];
}
/** Case-sensitive query lookup that falls back to searchParams */
export function getQuery(req: IncomingRequest, key: string): string | undefined {
  if (req.invalid || !req.query || !req.url) return undefined;
  // ✅ look in the parser-built map first
  const direct = req.query?.[key];
  if (direct !== undefined) return direct;
  // fallback (rare) – parse from URL
  return req.url.searchParams.get(key) ?? undefined;
}
</file>

<file path="utils/logger.ts">
/* eslint-disable prefer-const */
/* eslint-disable @typescript-eslint/no-unused-vars */
/* eslint-disable @typescript-eslint/no-explicit-any */
// logger.ts - Refactored modular and extensible logging utility
// Fix: Added explicit method signatures for standard levels + success to satisfy TypeScript.
import fs from 'fs';
import path from 'path';
import chalk from 'chalk'; // Dependency for PrettyFormatter
import boxen from 'boxen'; // Dependency for PrettyFormatter
import { Writable } from 'stream';
// --- Configuration ---
const standardLevels = {
  error: 0,
  warn: 1,
  info: 2,
  http: 3,
  verbose: 4,
  debug: 5,
  silly: 6,
};
type LogLevel = keyof typeof standardLevels;
type CustomLevels = Record<string, number>;
// Combine standard and potential custom levels for type checking
type AllLogLevels = LogLevel | 'success' | keyof CustomLevels; // Include 'success' explicitly if used often
// --- Interfaces ---
export interface LogEntry {
  level: string;
  message: string | object;
  meta?: Record<string, any>;
  timestamp: Date;
}
export interface Formatter {
  format(entry: LogEntry): string;
}
export interface Transport {
  log(formattedMessage: string, entry: LogEntry): void;
  level?: string;
  close?(): void;
  // Add formatter property to the interface for type safety
  formatter: Formatter;
}
// --- Formatters ---
export class JsonFormatter implements Formatter {
  format(entry: LogEntry): string {
    const logObject = {
      level: entry.level,
      message: entry.message,
      timestamp: entry.timestamp.toISOString(),
      ...entry.meta,
    };
    try {
      // Handle potential circular references and BigInts
      return JSON.stringify(logObject, (key, value) =>
        typeof value === 'bigint' ? value.toString() : value,
      );
    } catch (error) {
      // Fallback for stringification errors (e.g., circular refs)
      return JSON.stringify({
        level: entry.level,
        message: `[Unserializable Object: ${
          error instanceof Error ? error.message : String(error)
        }]`,
        timestamp: entry.timestamp.toISOString(),
        ...entry.meta,
      });
    }
  }
}
export class PrettyFormatter implements Formatter {
  private readonly options: {
    useColors: boolean;
    useBoxes: boolean;
    maxDepth: number;
    indent: number;
    stringLengthLimit: number;
    arrayLengthLimit: number;
    objectKeysLimit: number;
    showTimestamp: boolean;
  };
  private static LANGUAGE_COLOR_MAP: Record<string, (s: string) => string> = {
    python: chalk.magentaBright,
    typescript: chalk.cyanBright,
    javascript: chalk.yellowBright,
    shell: chalk.greenBright,
    default: chalk.whiteBright,
  };
  private static LEVEL_STYLES: Record<
    string,
    { color: (s: string) => string; icon: string; colorName: string }
  > = {
    error: { color: chalk.red, icon: '✖', colorName: 'red' },
    warn: { color: chalk.yellow, icon: '⚠', colorName: 'yellow' },
    info: { color: chalk.blueBright, icon: 'ℹ', colorName: 'blueBright' },
    success: { color: chalk.green, icon: '✔', colorName: 'green' },
    http: { color: chalk.magenta, icon: '↔', colorName: 'magenta' },
    verbose: { color: chalk.gray, icon: ' V ', colorName: 'gray' },
    debug: { color: chalk.cyan, icon: ' D ', colorName: 'cyan' },
    silly: { color: chalk.white, icon: ' S ', colorName: 'white' },
    default: { color: chalk.white, icon: ' ', colorName: 'white' },
  };
  constructor(options: Partial<PrettyFormatter['options']> = {}) {
    this.options = {
      useColors: options.useColors ?? true,
      useBoxes: options.useBoxes ?? false,
      maxDepth: options.maxDepth ?? 3, // Increased default depth slightly
      indent: options.indent ?? 2,
      stringLengthLimit: options.stringLengthLimit ?? 150, // Increased limit slightly
      arrayLengthLimit: options.arrayLengthLimit ?? 5,
      objectKeysLimit: options.objectKeysLimit ?? 5,
      showTimestamp: options.showTimestamp ?? false,
    };
  }
  private detectLanguage(context?: string): string {
    if (!context) return 'default';
    if (context.endsWith('.py')) return 'python';
    if (context.endsWith('.ts')) return 'typescript';
    if (context.endsWith('.js')) return 'javascript';
    if (context.endsWith('.sh')) return 'shell';
    return 'default';
  }
  private highlightSemantics(str: string): string {
    // Avoid double-coloring numbers if useBoxes is true (boxen will color the whole box)
    if (!this.options.useColors) return str;
    try {
      const hasAnsi = new RegExp(
        // eslint-disable-next-line no-control-regex
        '(?:\\u001b\\[[0-9;]*m|\\x1b\\[[0-9;]*m|\\u001b\\[.*?m|\\x1b\\[.*?m)',
      ).test(str);
      // Highlight file paths
      str = str.replace(
        /([./\w-]+\.(?:ts|js|py|sh|json|log|txt|md|html|css))/g,
        chalk.underline.blue('$1'),
      );
      // Highlight error keywords
      str = str.replace(
        /\b(error|exception|fail(?:ed)?|traceback|stack|warn(?:ing)?|critical|fatal)\b/gi,
        chalk.bgRed.white('$1'),
      );
      // Only highlight numbers if not using boxes (to avoid double-coloring)
      if (!this.options.useBoxes && !hasAnsi) {
        str = str.replace(/\b(\d+(?:\.\d+)?)\b/g, chalk.yellow('$1'));
      }
      // Highlight common log levels in messages
      str = str.replace(/\b(INFO|DEBUG|WARN|ERROR|SUCCESS|HTTP|VERBOSE|SILLY)\b/g, (match) => {
        const style = PrettyFormatter.LEVEL_STYLES[match.toLowerCase()] || {
          color: chalk.white,
        };
        return style.color(match);
      });
    } catch (e) {
      console.error('PrettyFormatter highlighting error:', e);
    }
    return str;
  }
  private formatValue(value: any, level = 0): string {
    const pad = ' '.repeat(this.options.indent * level);
    const colorize = this.options.useColors;
    // Max depth check
    if (level > this.options.maxDepth) {
      if (Array.isArray(value)) return colorize ? chalk.gray('[Array]') : '[Array]';
      if (typeof value === 'object' && value !== null)
        return colorize ? chalk.gray('[Object]') : '[Object]';
      return colorize ? chalk.gray('...') : '...';
    }
    // Type-based formatting
    if (value === null) return colorize ? chalk.gray('null') : 'null';
    if (value === undefined) return colorize ? chalk.gray('undefined') : 'undefined';
    if (typeof value === 'string') {
      const highlighted = this.highlightSemantics(value);
      const truncated =
        highlighted.length > this.options.stringLengthLimit
          ? highlighted.slice(0, this.options.stringLengthLimit) + '...'
          : highlighted;
      // Use JSON.stringify for proper escaping, then remove surrounding quotes for readability
      const jsonStr = JSON.stringify(truncated);
      return colorize ? chalk.yellowBright(jsonStr.slice(1, -1)) : jsonStr.slice(1, -1);
    }
    if (typeof value === 'number')
      return colorize ? chalk.green(value.toString()) : value.toString();
    if (typeof value === 'boolean')
      return colorize ? chalk.magenta(value.toString()) : value.toString();
    if (typeof value === 'bigint')
      return colorize ? chalk.greenBright(value.toString() + 'n') : value.toString() + 'n';
    if (typeof value === 'function')
      return colorize ? chalk.blueBright('[Function]') : '[Function]';
    if (value instanceof Date)
      return colorize ? chalk.greenBright(value.toISOString()) : value.toISOString();
    if (value instanceof Error) {
      const stack = value.stack ? `\n${value.stack.split('\n').slice(1).join('\n')}` : ''; // Show stack excluding first line
      const formattedError = `${value.name}: ${value.message}${stack}`;
      return colorize ? chalk.redBright(this.highlightSemantics(formattedError)) : formattedError;
    }
    if (value instanceof RegExp)
      return colorize ? chalk.magentaBright(value.toString()) : value.toString();
    if (Array.isArray(value)) {
      if (value.length === 0) return '[]';
      let out = '[\n';
      const max = this.options.arrayLengthLimit;
      for (let i = 0; i < Math.min(value.length, max); i++) {
        out += pad + '  ' + this.formatValue(value[i], level + 1);
        if (i < Math.min(value.length, max) - 1 || value.length > max) out += ','; // Add comma if not last shown or if truncated
        out += '\n';
      }
      if (value.length > max) {
        const moreText = `... ${value.length - max} more item(s)`;
        out += pad + '  ' + (colorize ? chalk.gray(moreText) : moreText) + '\n';
      }
      out += pad + ']';
      return out;
    }
    if (typeof value === 'object' && value !== null) {
      // Handle potential circular references more gracefully during formatting
      try {
        const keys = Object.keys(value);
        if (keys.length === 0) return '{}';
        let out = '{\n';
        const max = this.options.objectKeysLimit;
        for (let idx = 0; idx < Math.min(keys.length, max); idx++) {
          const key = keys[idx];
          const keyStr = colorize ? chalk.cyanBright(`"${key}"`) : `"${key}"`;
          out += pad + '  ' + keyStr + ': ';
          out += this.formatValue(value[key], level + 1);
          if (idx < Math.min(keys.length, max) - 1 || keys.length > max) out += ','; // Add comma
          out += '\n';
        }
        if (keys.length > max) {
          const moreText = `... ${keys.length - max} more key(s)`;
          out += pad + '  ' + (colorize ? chalk.gray(moreText) : moreText) + '\n';
        }
        out += pad + '}';
        return out;
      } catch (e) {
        // Likely a circular reference or other object traversal issue
        return colorize
          ? chalk.redBright('[Object Formatting Error]')
          : '[Object Formatting Error]';
      }
    }
    // Fallback for other types
    return colorize ? chalk.white(String(value)) : String(value);
  }
  format(entry: LogEntry): string {
    const { level, message, meta, timestamp } = entry;
    const style = PrettyFormatter.LEVEL_STYLES[level] || PrettyFormatter.LEVEL_STYLES.default;
    // When using boxes, do not color the inner message at all
    let formattedMessage = typeof message === 'string' ? message : JSON.stringify(message);
    let formattedMeta = '';
    if (meta && Object.keys(meta).length > 0 && meta !== message) {
      formattedMeta = typeof meta === 'string' ? meta : JSON.stringify(meta);
    }
    let combinedOutput = formattedMessage;
    if (formattedMeta && formattedMeta !== '{}') {
      combinedOutput += `\nMeta: ${formattedMeta}`;
    }
    const timestampStr = this.options.showTimestamp ? `[${timestamp.toISOString()}] ` : '';
    const levelStr = `${style.icon} ${level.toUpperCase()} `;
    let finalMessage = `${timestampStr}${levelStr}${combinedOutput}`;
    if (this.options.useBoxes && this.options.useColors) {
      // Only color the box, not the content
      return boxen(finalMessage, {
        padding: 1,
        margin: { top: 0, bottom: 1, left: 0, right: 0 },
        borderStyle: 'round',
        borderColor: style.colorName,
        backgroundColor: undefined,
        title: undefined,
        titleAlignment: 'center',
      });
    } else if (this.options.useColors) {
      return style.color(finalMessage);
    } else {
      return finalMessage;
    }
  }
}
// --- Transports ---
export class ConsoleTransport implements Transport {
  public formatter: Formatter;
  public level?: string;
  constructor(options: { formatter?: Formatter; level?: string } = {}) {
    this.formatter = options.formatter ?? new PrettyFormatter({ useColors: true, useBoxes: false });
    this.level = options.level;
  }
  log(formattedMessage: string, entry: LogEntry): void {
    if (entry.level === 'error') {
      console.error(formattedMessage);
    } else if (entry.level === 'warn') {
      console.warn(formattedMessage);
    } else {
      console.log(formattedMessage);
    }
  }
}
export class FileTransport implements Transport {
  public formatter: Formatter;
  private stream: Writable;
  private filename: string;
  public level?: string;
  constructor(options: { filename: string; formatter?: Formatter; level?: string }) {
    this.filename = options.filename;
    // Default to JSON for files unless overridden
    this.formatter = options.formatter ?? new JsonFormatter();
    this.level = options.level;
    this.ensureLogDir(this.filename);
    this.stream = fs.createWriteStream(this.filename, { flags: 'a' });
    this.stream.on('error', (err) => {
      console.error(`Error writing to log file ${this.filename}:`, err);
    });
    // Handle stream closing gracefully
    this.stream.on('finish', () => {
      // console.log(`Log stream closed for ${this.filename}`);
    });
  }
  private ensureLogDir(logPath: string): void {
    const dir = path.dirname(logPath);
    try {
      if (!fs.existsSync(dir)) {
        fs.mkdirSync(dir, { recursive: true });
      }
    } catch (err) {
      console.error(`Failed to create log directory ${dir}:`, err);
    }
  }
  log(formattedMessage: string, entry: LogEntry): void {
    this.stream.write(formattedMessage + '\n', (err) => {
      if (err) {
        console.error(`Failed to write to log stream ${this.filename}:`, err);
      }
    });
  }
  close(): void {
    // Promisify stream end for cleaner shutdown
    new Promise<void>((resolve) => {
      this.stream.end(() => resolve());
    }).catch((err) => {
      console.error(`Error closing log stream ${this.filename}:`, err);
    });
  }
}
// --- Logger Core ---
export interface LoggerOptions {
  level?: LogLevel | string;
  levels?: CustomLevels;
  transports?: Transport[];
  metadata?: Record<string, any>;
  exitOnError?: boolean;
}
export class Logger {
  private options: Required<Omit<LoggerOptions, 'levels'>>; // Omit levels as it's merged into this.levels
  private levels: Record<string, number>;
  private transports: Transport[];
  // --- TypeScript Fix: Explicit Method Signatures ---
  // Define methods for standard levels + 'success' so TypeScript knows they exist.
  // The actual implementation is still handled dynamically below or via the core `log` method.
  error!: (message: string | object, meta?: Record<string, any>) => void;
  warn!: (message: string | object, meta?: Record<string, any>) => void;
  info!: (message: string | object, meta?: Record<string, any>) => void;
  http!: (message: string | object, meta?: Record<string, any>) => void;
  verbose!: (message: string | object, meta?: Record<string, any>) => void;
  debug!: (message: string | object, meta?: Record<string, any>) => void;
  silly!: (message: string | object, meta?: Record<string, any>) => void;
  success!: (message: string | object, meta?: Record<string, any>) => void; // Include success if it's commonly used
  // --- End TypeScript Fix ---
  constructor(options: LoggerOptions = {}) {
    // Define levels, merging standard and custom. Ensure 'success' is present if used.
    this.levels = {
      ...standardLevels,
      success: standardLevels.info,
      ...(options.levels ?? {}),
    }; // Map success to info level by default
    const defaultTransports = options.transports ?? [
      new ConsoleTransport({
        formatter: new PrettyFormatter({ useColors: true }),
      }),
    ];
    this.options = {
      level: options.level ?? 'info',
      transports: defaultTransports,
      metadata: options.metadata ?? {},
      exitOnError: options.exitOnError ?? false,
    };
    this.transports = this.options.transports;
    // --- Dynamic Method Implementation ---
    // This part still creates the runtime methods, but TypeScript now relies on the explicit signatures above.
    Object.keys(this.levels).forEach((level) => {
      // Assign the implementation to the pre-declared properties
      (this as any)[level] = (message: string | object, meta?: Record<string, any>) => {
        this.log(level, message, meta);
      };
    });
  }
  log(level: string, message: string | object, meta?: Record<string, any>): void {
    const levelValue = this.levels[level as string] ?? -1;
    const configuredLevelValue = this.levels[this.options.level as string] ?? this.levels.info;
    if (levelValue === -1) {
      console.warn(`Attempted to log with unknown level: "${level}"`);
      return; // Don't log unknown levels
    }
    // Check main logger level
    if (levelValue > configuredLevelValue) {
      return;
    }
    const entry: LogEntry = {
      level,
      message,
      meta: { ...this.options.metadata, ...meta },
      timestamp: new Date(),
    };
    this.transports.forEach((transport) => {
      const transportLevel = transport.level ?? this.options.level;
      const transportLevelValue =
        this.levels[transportLevel as Exclude<keyof AllLogLevels, symbol>] ?? configuredLevelValue;
      // Check transport-specific level
      if (typeof transportLevelValue === 'number' && levelValue <= transportLevelValue) {
        // Use the formatter associated with the transport
        const formattedMessage = transport.formatter.format(entry);
        try {
          transport.log(formattedMessage, entry);
        } catch (err) {
          console.error(`Error in transport ${transport.constructor.name}:`, err);
        }
      }
    });
  }
  child(metadata: Record<string, any>): Logger {
    // Create a new instance, inheriting options but merging metadata
    const childOptions: LoggerOptions = {
      level: this.options.level,
      levels: this.levels, // Pass the combined levels object
      transports: this.transports, // Share transports by default
      metadata: { ...this.options.metadata, ...metadata },
      exitOnError: this.options.exitOnError,
    };
    return new Logger(childOptions);
  }
  close(): void {
    // Use Promise.all to wait for all streams to close
    Promise.all(
      this.transports.map((transport) => {
        if (typeof transport.close === 'function') {
          try {
            return transport.close(); // Assuming close might return a promise or be synchronous
          } catch (err) {
            console.error(`Error closing transport ${transport.constructor.name}:`, err);
            return Promise.resolve(); // Resolve even if one fails to close others
          }
        }
        return Promise.resolve();
      }),
    ).catch((err) => {
      console.error('Error during logger close:', err);
    });
  }
  // --- Placeholder for AI Integration ---
  // ... (keep the AI placeholder comment)
}
// --- Backward Compatibility & Default Export ---
const defaultLogFilePath = process.env.LOG_FILE_PATH || path.join(process.cwd(), 'logs/app.log');
const defaultLogger = new Logger({
  level: (process.env.LOG_LEVEL as LogLevel) || 'info',
  transports: [
    new ConsoleTransport({
      formatter: new PrettyFormatter({
        useColors: true,
        useBoxes: true, // Keep boxes for default console
        showTimestamp: false,
      }),
      level: (process.env.CONSOLE_LOG_LEVEL as LogLevel) || undefined,
    }),
    new FileTransport({
      filename: defaultLogFilePath,
      formatter: new PrettyFormatter({
        useColors: false,
        useBoxes: false, // No boxes for file transport
        showTimestamp: true,
      }), // Default file transport to JSON
      level: (process.env.FILE_LOG_LEVEL as LogLevel) || undefined,
    }),
  ],
  // levels definition already includes 'success' mapped to 'info' in the constructor
});
export default defaultLogger;
export { standardLevels };
// --- Usage Examples ---
</file>

<file path="utils/mimeTypes.ts">
export const mimeTypes: Record<string, string> = {
  '.html': 'text/html',
  '.htm': 'text/html',
  '.js': 'application/javascript',
  '.json': 'application/json',
  '.css': 'text/css',
  '.txt': 'text/plain',
  '.jpg': 'image/jpeg',
  '.jpeg': 'image/jpeg',
  '.png': 'image/png',
  '.gif': 'image/gif',
  '.svg': 'image/svg+xml',
  '.ico': 'image/x-icon',
  '.mp3': 'audio/mpeg',
  '.mp4': 'video/mp4',
  '.webm': 'video/webm',
  '.ogg': 'audio/ogg',
  '.pdf': 'application/pdf',
  '.zip': 'application/zip',
  '.tar': 'application/x-tar',
};
</file>

<file path="main.ts">
// main.ts
import './routes';
import { HttpServer } from './core/server';
import { config } from './config/server.config';
import logger from './utils/logger';
logger.child([JSON.stringify(config, null, 2)]).info('routes loaded');
const server = new HttpServer(config.port);
server.start();
</file>

<file path="embedding_service_helper.py">
#!/usr/bin/env python
"""
get_clip_embedding.py – now supports batching & on-disk caching
Usage:
    python get_clip_embedding.py <file_paths> [--num_frames NUM] [--model MODEL] [--debug]
This script computes CLIP embeddings for one or more image or video files.
For video files, it extracts multiple frames selected via an advanced, content-aware sampling
strategy that combines scene-representative sampling with visual entropy-based salience and
temporal smoothing. The embeddings are computed in batch and averaged.
The script uses the Hugging Face Transformers library for CLIP model loading and inference,
and ffmpeg for video processing. It supports caching to avoid redundant computation.
Features:
    • Batch processing of multiple files.
    • On-disk caching to avoid redundant computation.
    • Image and video file support (JPEG, PNG, MP4, etc.).
    • Advanced video frame extraction using scene detection and visual entropy.
    • CLIP model loading and inference using Hugging Face Transformers.
    • Inference on CPU or GPU (if available).
    • Configurable number of frames for video processing.
    • Command-line arguments for file paths, model name, number of frames, and debug metadata.
    • JSON output format for embedding and debug metadata.
    • Error handling and logging.
    • Debug metadata output for advanced sampling methods.
    • Support for multiple CLIP models from Hugging Face.
    • Parallel frame extraction for performance.
    • In-memory frame extraction without temporary file I/O.
    • Modular design with classes for video processing and CLIP inference.
    • Configurable parameters via command-line arguments.
    • Structured logging and centralized error handling.
    • Scene detection via PySceneDetect (if installed) to extract scene midpoints.
    • Visual entropy computation as a salience heuristic.
    • Temporal diversity enforcement (smoothing) via a minimum time-gap filter.
    • Fallback to uniform sampling if advanced scene analysis fails.
    • Optional debug metadata (candidate timestamps, entropy values, selected timestamps).
    • Optional data augmentation for images and video frames.
Performance and production readiness improvements include:
    • Parallel frame extraction using concurrent futures.
    • In-memory frame extraction without temporary file I/O.
    • Modular design with classes for video processing and CLIP inference.
    • Configurable parameters via command-line arguments.
    • Structured logging and centralized error handling.
Advanced improvements in this version:
    • Scene detection via PySceneDetect (if installed) to extract scene midpoints.
    • Visual entropy computation as a salience heuristic.
    • Temporal diversity enforcement (smoothing) via a minimum time-gap filter.
    • Fallback to uniform sampling if advanced scene analysis fails.
    • Optional debug metadata (candidate timestamps, entropy values, selected timestamps).
    • Optional data augmentation for images and video frames.
"""
import logging
import sys
import os
import json
from PIL import Image
import torch
import signal
PY_LOG_PREFIX = "[PyEmbeddingHelper]"
class StructuredFormatter(logging.Formatter):
    def format(self, record):
        base = {
            "prefix": PY_LOG_PREFIX,
            "level": record.levelname,
            "time": self.formatTime(record, self.datefmt),
            "file": record.pathname,
            "func": record.funcName,
            "line": record.lineno,
            "msg": record.getMessage(),
        }
        if record.exc_info:
            base["exc"] = self.formatException(record.exc_info)
        return json.dumps(base)
# Setup root logger for both console and file
console_handler = logging.StreamHandler(sys.stderr)
console_handler.setFormatter(
    logging.Formatter(
        f"{PY_LOG_PREFIX} %(asctime)s %(levelname)s %(funcName)s: %(message)s",
        "%Y-%m-%d %H:%M:%S",
    )
)
log_file_path = os.path.join(
    os.path.dirname(__file__), "._" + os.path.basename(__file__) + ".log"
)
file_handler = logging.FileHandler(log_file_path)
file_handler.setFormatter(StructuredFormatter())
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # Set to DEBUG for maximum output
root_logger.handlers = [console_handler, file_handler]
logging.getLogger("transformers").setLevel(logging.CRITICAL)
logging.getLogger("urllib3").setLevel(logging.CRITICAL)
logging.getLogger("huggingface_hub").setLevel(logging.CRITICAL)
logging.getLogger().setLevel(logging.ERROR)
def handle_sigint(signum, frame):
    logging.getLogger(__name__).info("Received SIGINT (Ctrl+C). Exiting gracefully.")
    # Perform any additional cleanup here if needed
    sys.exit(0)
def handle_sigterm(signum, frame):
    logging.getLogger(__name__).info("Received SIGTERM. Exiting gracefully.")
    sys.exit(0)
signal.signal(signal.SIGINT, handle_sigint)
signal.signal(signal.SIGTERM, handle_sigterm)
from transformers import CLIPProcessor, CLIPModel
import math
import subprocess
import concurrent.futures
import io
from contextlib import nullcontext
# Supported file extensions for images and videos.
IMAGE_EXTS = [".jpg", ".jpeg", ".png", ".webp", ".avif", ".gif"]
VIDEO_EXTS = [".mp4", ".mov", ".webm", ".ogg", ".m4v"]
def compute_entropy(image: Image.Image) -> float:
    """
    Compute the visual entropy of an image as a simple salience measure.
    The image is converted to grayscale; its normalized histogram is used to
    calculate entropy (in bits).
    Args:
        image (PIL.Image): The input image.
    Returns:
        float: The computed entropy value.
    """
    grayscale = image.convert("L")
    histogram = grayscale.histogram()
    total_pixels = sum(histogram)
    entropy = 0.0
    # Compute entropy as sum(-p * log2(p)) for nonzero probabilities.
    for count in histogram:
        if count > 0:
            p = count / total_pixels
            entropy -= p * math.log(p, 2)
    return entropy
class VideoProcessor:
    """
    A class to manage video processing tasks like duration extraction and advanced
    frame extraction using a combination of scene detection, visual entropy, and
    temporal smoothing.
    The class uses ffmpeg for video processing and PIL for image handling.
    It also provides methods for extracting frames based on advanced sampling
    strategies, including scene detection and visual entropy salience.
    The class is designed to be modular and reusable, allowing for easy integration
    into larger systems or pipelines.
    Args:
        video_path (str): Path to the video file.
        num_frames (int): Number of frames to sample from the video.
        logger (logging.Logger): Optional logger for debug information.
        executor (concurrent.futures.ThreadPoolExecutor): Shared executor for parallel frame extraction.
    Attributes:
        video_path (str): Path to the video file.
        num_frames (int): Number of frames to sample from the video.
        logger (logging.Logger): Logger for debug information.
        duration (float): Duration of the video in seconds.
        executor (concurrent.futures.ThreadPoolExecutor): Shared executor for parallel frame extraction.
    Methods:
        get_duration(): Get the duration of the video using ffprobe.
        get_advanced_sample_times(): Compute candidate sampling timestamps using
            scene detection, visual entropy, and temporal smoothing.
        extract_frame(time_sec): Extract a single frame from the video at a
            specific time (in seconds).
        extract_frames(): Extract frames concurrently based on advanced sampling times.
        compute_entropy(image): Compute the visual entropy of an image.
    """
    def __init__(self, video_path: str, num_frames: int, logger=None, executor=None):
        self.video_path = video_path
        self.num_frames = num_frames
        self.logger = logger or logging.getLogger(__name__)
        self.duration = self.get_duration()
        self.executor = executor
    def get_duration(self) -> float:
        """Get the duration of the video in seconds using ffprobe.
        Returns:
            float: Duration of the video in seconds.
        Raises:
            RuntimeError: If ffprobe fails to retrieve the duration.
        """
        self.logger.debug(
            f"Calling ffprobe to get duration for video: {self.video_path}"
        )
        try:
            result = subprocess.run(
                [
                    "ffprobe",
                    "-v",
                    "error",
                    "-show_entries",
                    "format=duration",
                    "-of",
                    "default=noprint_wrappers=1:nokey=1",
                    self.video_path,
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=True,
            )
            duration = float(result.stdout.strip())
            self.logger.debug(f"Video duration: {duration} seconds")
            return duration
        except Exception as e:
            self.logger.error("Failed to get video duration", exc_info=True)
            raise RuntimeError(f"Failed to get video duration: {e}")
    def get_advanced_sample_times(self) -> tuple:
        """
        Compute candidate sampling timestamps using a combination of scene detection,
        visual entropy salience, and temporal smoothing. The method first attempts
        to extract scene boundaries via PySceneDetect. If successful, it computes a
        candidate timestamp for each scene (using the midpoint). For each candidate, a
        frame is extracted to compute visual entropy. Candidates are then ranked and
        filtered to enforce a minimum time gap (diversity). If scene detection fails or
        does not yield enough candidates, the method falls back to uniform sampling.
        The method returns a tuple containing the selected timestamps and debug metadata
        (if requested).
        Args:
            None
        Returns:
            tuple: (selected_timestamps, debug_metadata)
                selected_timestamps (list): List of selected timestamps for frame extraction.
                debug_metadata (dict): Debug metadata containing candidate timestamps,
                                       entropy values, and method used.
        Raises:
            RuntimeError: If frame extraction fails or no frames are extracted.
        """
        candidate_times = None
        debug_metadata = {}
        method_used = ""
        # Attempt scene detection with PySceneDetect.
        try:
            from scenedetect import VideoManager, SceneManager
            from scenedetect.detectors import ContentDetector
            video_manager = VideoManager([self.video_path])
            scene_manager = SceneManager()
            # The threshold here is heuristic; adjust based on your domain.
            scene_manager.add_detector(ContentDetector(threshold=25, min_scene_len=10))
            video_manager.start()
            scene_manager.detect_scenes(frame_source=video_manager, show_progress=True)
            scene_list = scene_manager.get_scene_list()
            candidate_times = [
                (scene[0].get_seconds() + scene[1].get_seconds()) / 2
                for scene in scene_list
            ]
            debug_metadata["scene_count"] = len(scene_list)
            method_used = "scene_detection"
            self.logger.debug(f"Detected {len(scene_list)} scenes.")
            # If scene coverage is insufficient or the video is too short, fall back to uniform sampling.
            if (
                len(candidate_times)
                < self.num_frames  # fewer scene midpoints than needed frames
                or len(scene_list) < (self.num_frames // 2)  # too few distinct scenes
                or self.duration
                < (self.num_frames * 2)  # video too short for advanced sampling
            ):
                self.logger.warning(
                    "Insufficient scene coverage or short video; falling back to uniform sampling."
                )
                candidate_times = None
        except Exception as e:
            self.logger.warning(
                "Scene detection failed or PySceneDetect not installed; falling back to dense candidate extraction.",
                exc_info=True,
            )
        # If candidate times are insufficient, fall back to uniform dense extraction.
        if not candidate_times or len(candidate_times) < self.num_frames:
            candidate_times = [
                (i + 1) * self.duration / (self.num_frames + 1)
                for i in range(self.num_frames)
            ]
            method_used = "fallback_uniform"
            debug_metadata = {"method_used": method_used, "timestamps": candidate_times}
            return candidate_times, debug_metadata
        # For each candidate timestamp, extract the frame and compute visual entropy.
        candidate_frames = []
        executor = self.executor or concurrent.futures.ThreadPoolExecutor()
        with executor if self.executor is None else nullcontext(executor):
            future_to_time = {
                executor.submit(self.extract_frame, t): t for t in candidate_times
            }
            for future in concurrent.futures.as_completed(future_to_time):
                t = future_to_time[future]
                try:
                    frame = future.result()
                    candidate_frames.append((t, frame))
                except Exception as e:
                    self.logger.error(
                        f"Failed to extract candidate frame at {t} sec: {e}"
                    )
        if not candidate_frames:
            # If extraction completely failed, fallback.
            uniform_times = [
                (i + 1) * self.duration / (self.num_frames + 1)
                for i in range(self.num_frames)
            ]
            debug_metadata = {
                "method_used": "fallback_uniform_extraction",
                "timestamps": uniform_times,
            }
            return uniform_times, debug_metadata
        # Compute entropy values for all candidate frames.
        entropy_values = []
        for t, frame in candidate_frames:
            try:
                entropy_val = compute_entropy(frame)
            except Exception as e:
                entropy_val = 0.0
            entropy_values.append((t, entropy_val))
        debug_metadata["entropy_values"] = entropy_values
        # Rank candidates by entropy (descending).
        entropy_values.sort(key=lambda x: x[1], reverse=True)
        # Select frames ensuring temporal diversity.
        selected = []
        diversity_threshold = (
            self.duration * 0.05
        )  # At least 5% of video duration apart.
        for t, entropy_val in entropy_values:
            if not selected or all(abs(t - s) > diversity_threshold for s in selected):
                selected.append(t)
            if len(selected) == self.num_frames:
                break
        # In case diversity filtering does not yield enough frames, fill with lower-ranked ones.
        debug_metadata["selected_entropy_values"] = [
            (t, entropy_val) for t, entropy_val in entropy_values if t in selected
        ]
        if len(selected) < self.num_frames:
            remaining = sorted([t for t, _ in entropy_values if t not in selected])
            for t in remaining:
                if len(selected) < self.num_frames:
                    selected.append(t)
        selected.sort()
        debug_metadata["selected_times"] = selected
        debug_metadata["method_used"] = method_used
        return selected, debug_metadata
    def extract_frame(self, time_sec: float) -> Image.Image:
        """
        Extract a single frame from the video at a specific time (in seconds)
        by piping JPEG data through stdout.
        Args:
            time_sec (float): Time in seconds to extract the frame.
        Returns:
            PIL.Image: The extracted frame as a PIL Image object.
        Raises:
            RuntimeError: If ffmpeg fails to extract the frame.
        """
        command = [
            "ffmpeg",
            "-y",
            "-ss",
            str(time_sec),
            "-i",
            self.video_path,
            "-vframes",
            "1",
            "-f",
            "image2pipe",
            "-vcodec",
            "mjpeg",
            "-",
        ]
        self.logger.debug(
            f"Calling ffmpeg to extract frame at {time_sec} seconds from video: {self.video_path}"
        )
        try:
            result = subprocess.run(
                command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True
            )
            if not result.stdout:
                raise RuntimeError("No frame data returned from ffmpeg.")
            image = Image.open(io.BytesIO(result.stdout)).convert("RGB")
            self.logger.debug(f"Extracted frame at {time_sec} seconds")
            return image
        except Exception as e:
            self.logger.error(
                f"Frame extraction failed at {time_sec} sec", exc_info=True
            )
            raise RuntimeError(f"Failed to extract frame at {time_sec} sec: {e}")
    def extract_frames(self) -> tuple:
        """
        Extract frames concurrently based on advanced sampling times.
        Returns a tuple: (list of PIL.Image objects, debug_metadata)
        The frames are extracted using the advanced sampling method, which
        combines scene detection, visual entropy, and temporal smoothing.
        Args:
            None
        Returns:
            tuple: (frames, debug_metadata)
                frames (list): List of extracted frames as PIL Image objects.
                debug_metadata (dict): Debug metadata containing candidate timestamps,
                                       entropy values, and method used.
        Raises:
            RuntimeError: If frame extraction fails or no frames are extracted.
        """
        timestamps, debug_metadata = self.get_advanced_sample_times()
        frames = []
        executor = self.executor or concurrent.futures.ThreadPoolExecutor()
        with executor if self.executor is None else nullcontext(executor):
            future_to_time = {
                executor.submit(self.extract_frame, t): t for t in timestamps
            }
            for future in concurrent.futures.as_completed(future_to_time):
                t = future_to_time[future]
                try:
                    frame = future.result()
                    frames.append((t, frame))
                except Exception as e:
                    self.logger.error(f"Failed to extract frame at {t} sec: {e}")
        if not frames:
            raise RuntimeError("No frames extracted from video.")
        frames.sort(key=lambda x: x[0])
        return [frame for _, frame in frames], debug_metadata
class CLIPEmbedder:
    """
    A class to handle CLIP model loading and embedding computation for images and videos.
    The class uses the Hugging Face Transformers library to load the CLIP model and
    processor. It provides methods for computing embeddings for both single images and
    averaged embeddings from multiple video frames.
    Args:
        model_name (str): Name of the CLIP model to load from Hugging Face.
        device (str): Device to run the model on ('mps', 'cuda', or 'cpu').
        logger (logging.Logger): Optional logger for debug information.
        enable_augmentation (bool): If True, apply basic image augmentations.
    Attributes:
        model_name (str): Name of the CLIP model.
        device (str): Device to run the model on.
        logger (logging.Logger): Logger for debug information.
        model (CLIPModel): Loaded CLIP model.
        processor (CLIPProcessor): Processor for pre-processing inputs.
        enable_augmentation (bool): Flag to enable data augmentation.
        augmentation_transforms (torchvision.transforms.Compose): Data augmentation transforms.
    Methods:
        get_image_embedding(image): Compute and return the normalized CLIP embedding for a single image.
        get_video_embedding(frames): Compute and return an averaged CLIP embedding from multiple video frames.
    """
    def __init__(
        self,
        model_name="openai/clip-vit-base-patch32",
        device=None,
        logger=None,
        enable_augmentation=False,
    ):
        self.logger = logger
        # Device selection: Prefer MPS on M1 Mac, then CUDA, then CPU.
        if device is None:
            if torch.backends.mps.is_available():
                self.device = "mps"
            elif torch.cuda.is_available():
                self.device = "cuda"
            else:
                self.device = "cpu"
        else:
            self.device = device
        self.model = CLIPModel.from_pretrained(model_name).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        # Optional data augmentation transforms for images.
        self.enable_augmentation = enable_augmentation
        if enable_augmentation:
            import torchvision.transforms as T
            self.augmentation_transforms = T.Compose(
                [
                    T.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),
                    T.RandomHorizontalFlip(p=0.5),
                ]
            )
        else:
            self.augmentation_transforms = None
    def get_image_embedding(self, image: Image.Image) -> list:
        """
        Compute and return the normalized CLIP embedding for a single image,
        optionally applying data augmentation.
        """
        if self.enable_augmentation:
            image = self.augmentation_transforms(image)
        inputs = self.processor(images=image, return_tensors="pt").to(self.device)
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
        return image_features[0].cpu().numpy().tolist()
    def get_video_embedding(self, frames: list) -> list:
        """
        Compute and return an averaged CLIP embedding from multiple video frames.
        Future improvements can include weighted or attention-based aggregation.
        """
        # If augmentation is enabled, apply transforms to each frame.
        if self.enable_augmentation:
            frames = [self.augmentation_transforms(f) for f in frames]
        inputs = self.processor(images=frames, return_tensors="pt", padding=True).to(
            self.device
        )
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
        averaged_embedding = image_features.mean(dim=0)
        return averaged_embedding.cpu().numpy().tolist()
def compute_single_embedding(fp, args, embedder, executor=None):
    import logging
    import os
    import time
    ext = os.path.splitext(fp)[1].lower()
    logger = logging.getLogger(__name__)
    try:
        debug_metadata = {
            "model": getattr(args, "model", None),
            "num_frames": (
                getattr(args, "num_frames", None) if ext in VIDEO_EXTS else None
            ),
            "enable_augmentation": getattr(args, "enable_augmentation", False),
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        }
        if ext in VIDEO_EXTS:
            from io import BytesIO
            import subprocess
            logger.debug(f"Preparing to process video file: {fp}")
            video_processor = VideoProcessor(
                fp, args.num_frames, logger=logger, executor=executor
            )
            frames, adv_debug = video_processor.extract_frames()
            embedding = embedder.get_video_embedding(frames)
            debug_metadata.update(adv_debug)
            result = {
                "embedding": embedding,
                "debugMetadata": debug_metadata,
                "error": None,
                "detail": None,
            }
        elif ext in IMAGE_EXTS:
            logger.debug(f"Opening image file: {fp}")
            image = Image.open(fp).convert("RGB")
            embedding = embedder.get_image_embedding(image)
            result = {
                "embedding": embedding,
                "debugMetadata": debug_metadata,
                "error": None,
                "detail": None,
            }
        else:
            logger.error("Unsupported file type")
            result = {
                "embedding": None,
                "debugMetadata": debug_metadata,
                "error": "Unsupported file type",
                "detail": f"File type not supported: {ext} for file {fp}",
            }
        return result
    except Exception as e:
        logger.exception("An error occurred during processing.")
        return {
            "embedding": None,
            "debugMetadata": debug_metadata if "debug_metadata" in locals() else {},
            "error": "Processing failed",
            "detail": f"{type(e).__name__}: {e} (file: {fp})",
        }
def process_batch(image_paths, embedder, args, logger, shared_executor):
    results = {}
    total = len(image_paths)
    processed = 0
    for fp in image_paths:
        logger.debug(f"Processing file: {fp}")
        if not os.path.exists(fp):
            logger.error(f"Missing file: {fp}")
            results[fp] = {
                "embedding": None,
                "debugMetadata": {},
                "error": "File does not exist",
                "detail": f"Path not found: {fp}",
            }
            processed += 1
            print(
                f"PROGRESS: "
                + json.dumps({"processed": processed, "total": total, "current": fp}),
                file=sys.stderr,
                flush=True,
            )
            continue
        result = compute_single_embedding(fp, args, embedder, shared_executor)
        if "error" in result and "detail" not in result:
            result["detail"] = f"Failed to process file: {fp}"
        # Ensure all keys are present
        for k in ["embedding", "debugMetadata", "error", "detail"]:
            if k not in result:
                result[k] = None
        results[fp] = result
        processed += 1
        print(
            f"PROGRESS: "
            + json.dumps({"processed": processed, "total": total, "current": fp}),
            file=sys.stderr,
            flush=True,
        )
    return results
def main_service():
    import sys
    import argparse
    import logging
    parser = argparse.ArgumentParser(
        description="CLIP embedding service (stdin/stdout)"
    )
    parser.add_argument("--model", type=str, default="openai/clip-vit-base-patch32")
    parser.add_argument("-n", "--num_frames", type=int, default=20)
    parser.add_argument("--debug", action="store_true")
    parser.add_argument("--log", action="store_true")
    parser.add_argument("--enable_augmentation", action="store_true", default=False)
    args = parser.parse_args()
    logging.basicConfig(
        level=logging.DEBUG,
        format="[%(asctime)s] %(levelname)s %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        stream=sys.stderr,
    )
    logger = logging.getLogger(__name__)
    logger.info("CLIP embedding service started. Waiting for input...")
    embedder = CLIPEmbedder(
        model_name=args.model,
        device=None,
        logger=logger,
        enable_augmentation=args.enable_augmentation,
    )
    logger.info(f"CLIPEmbedder initialized on device: {embedder.device}")
    shared_executor = concurrent.futures.ThreadPoolExecutor()
    while True:
        try:
            line = sys.stdin.readline()
            if not line:
                break  # EOF
            line = line.strip()
            if not line:
                continue
            try:
                request = json.loads(line)
                image_paths = request.get("imagePaths")
                if not isinstance(image_paths, list):
                    raise ValueError("'imagePaths' must be a list.")
            except Exception as e:
                error_response = {"error": "Invalid input", "detail": str(e)}
                print(json.dumps(error_response), flush=True)
                continue
            try:
                results = process_batch(
                    image_paths, embedder, args, logger, shared_executor
                )
                print(json.dumps(results), flush=True)
            except Exception as e:
                # Instead of exiting, print a batchError JSON and continue
                batch_error = {
                    "batchError": str(e),
                    "detail": f"{type(e).__name__}: {e}",
                }
                print(json.dumps(batch_error), flush=True)
                continue
        except Exception as e:
            logger.error(f"Fatal error in main loop: {e}", exc_info=True)
            break
if __name__ == "__main__":
    main_service()
</file>

<file path="requirements.txt">
pillow==9.5.0
torch==2.1.0
scenedetect==0.7.0
torchvision==0.16.0
transformers==4.30.2
</file>

</files>
