# .env file for the Python Embedding Service

# --- Service Configuration ---
PYTHON_PORT=3456
# PYTHON_MEDIA_ROOT: Absolute path inside the Docker container where media is mounted.
# Node.js server will send file paths relative to this root.
# Example: If Node.js mediaDir is /mnt/nas/media and this is mounted to /media in Docker,
# and Node.js sends "photos/image.jpg", Python will look for "/media/photos/image.jpg".
PYTHON_MEDIA_ROOT=/app/public

# --- Logging Configuration ---
# Log level for the Python service (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=WARNING

# --- CLIP Model Configuration ---
# Full OpenCLIP model name (e.g., "openai/clip-vit-base-patch32", "laion/CLIP-ViT-B-32-laion2B-s34B-b79K")
# For "openai/clip-vit-base-patch32", helper parses to arch="ViT-B-32", pretrained="openai"
# For "laion/CLIP-ViT-B-32-laion2B-s34B-b79K", helper parses to arch="CLIP-ViT-B-32-laion2B-s34B-b79K", pretrained="laion" (adjust parsing if needed)
# Or more directly: "ViT-B-32/laion2b_s34b_b79k"
CLIP_MODEL="laion/CLIP-ViT-H-14-laion2B-s32B-b79K"

# Enable image augmentation ( "true" or "false" )
ENABLE_AUGMENTATION="true"

# --- Batching Manager Configuration ---
# Target VRAM utilization for dynamic batching (0.0 to 1.0, e.g., 0.85 for 85%)
TARGET_VRAM_UTILIZATION=0.80
# Maximum number of items in a single GPU processing batch (safeguard)
MAX_BATCH_ITEMS=128
# Timeout in seconds to wait for more items before flushing a smaller batch for GPU processing
BATCH_FLUSH_TIMEOUT_S=0.5
# Large item threshold in GB - items requiring more VRAM are processed separately
LARGE_ITEM_THRESHOLD_GB=0.5
# Maximum number of large items to process concurrently
MAX_CONCURRENT_LARGE_ITEMS=2
# Interval in seconds for the batching manager to poll for available VRAM and queue status when idle
GPU_POLL_INTERVAL_S=0.1

# --- Media Processing Configuration ---
# Default number of frames to extract from videos if not specified per item
DEFAULT_VIDEO_FRAMES_TO_EXTRACT=20
# Timeout in seconds for downloading media from URLs
DOWNLOAD_TIMEOUT_SECONDS=30

# --- Optional: Environment Variables for Python ---
# FFmpeg hardware acceleration method (e.g., cuda, vaapi, qsv, vulkan, amf, opencl). The script reads this at the start of batch processing
# FFMPEG_HWACCEL_METHOD=cuda

# --- Optional: Torch/CUDA specific settings (usually not needed here but can be set in environment) ---
# Example: CUDA_VISIBLE_DEVICES (if you want to restrict which GPUs are used, e.g., "0" or "0,1")
# CUDA_VISIBLE_DEVICES=